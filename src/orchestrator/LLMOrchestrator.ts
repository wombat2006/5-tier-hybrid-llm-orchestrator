import { 
  SystemConfig,
  LLMRequest,
  LLMResponse,
  BaseLLMClient,
  TaskType,
  ModelConfig,
  CollaborationRequest,
  CollaborationResponse,
  SystemMetrics
} from '../types';
import { ClaudeCodeQueryAnalyzer, ModelSuitabilityAnalyzer, QueryAnalysis } from '../analysis/QueryAnalyzer';
import { ContextAwareQueryAnalyzer } from '../analysis/ContextAwareQueryAnalyzer';
import { 
  Subtask, 
  DecompositionRequest, 
  DecompositionResult, 
  CollaborativeConfig, 
  CodingSession, 
  CodeResult,
  SubtaskStatus
} from '../types/collaborative';
import { ConfigLoader } from '../utils/ConfigLoader';
import { QwenCoderAPIClient } from '../clients/QwenCoderClient';
import { OpenRouterAPIClient } from '../clients/OpenRouterClient';
import { GeminiAPIClient } from '../clients/GeminiClient';
import { MockQwenClient } from '../clients/MockQwenClient';
import { AnthropicAPIClient } from '../clients/AnthropicClient';
import { OpenAIAPIClient } from '../clients/OpenAIClient';
import { OpenRouterModelRegistry } from '../services/OpenRouterModelRegistry';
import { ModelAliasResolver } from '../services/ModelAliasResolver';
// CLIé–¢é€£ã®importã‚’å‰Šé™¤ - ToolOrchestratorServiceã«ç§»è¡Œ
import { TaskDecomposer } from '../pipeline/TaskDecomposer';
import { DifficultyClassifier } from '../pipeline/DifficultyClassifier';
import { QualityGate } from '../pipeline/QualityGate';
import { PrecisionCostManagementSystem } from '../management/CostManagementSystem';
import { CostManagementSystem, TokenUsage } from '../types/cost-management';
import { DefaultCapabilityRegistry } from '../services/CapabilityRegistry';
import { CapabilityRegistry, CapabilityProvider } from '../types/capability';
import { ConversationManager } from '../services/ConversationManager';
import { OpenAIAssistantProvider } from '../services/OpenAIAssistantProvider';
import { AssistantConfig } from '../types/assistant';
import RedisLogger, { QueryAnalysisLog } from '../utils/RedisLogger';
import UpstashRedisLogger from '../utils/UpstashRedisLogger';
import LogAnalysisService, { LogAnalysisRequest } from '../services/LogAnalysisService';
import InteractiveTroubleshooter from '../services/InteractiveTroubleshooter';
import AdvancedLogAnalyzer, { LogAnalysisContext } from '../services/AdvancedLogAnalyzer';
import SafeExecutionManager from '../services/SafeExecutionManager';

export class LLMOrchestrator {
  private config: SystemConfig;
  private clients: Map<string, BaseLLMClient> = new Map();
  private metrics: SystemMetrics = {
    requests_per_tier: { 0: 0, 1: 0, 2: 0, 3: 0 },
    success_rate_per_tier: { 0: 0, 1: 0, 2: 0, 3: 0 },
    average_latency_per_tier: { 0: 0, 1: 0, 2: 0, 3: 0 },
    cost_per_tier: { 0: 0, 1: 0, 2: 0, 3: 0 },
    total_monthly_spend: 0,
    budget_utilization_percentage: 0,
    most_used_capabilities: [],
    error_distribution: {}
  };
  private monthlySpend: number = 0;
  private requestCount: number = 0;
  
  // å”èª¿ãƒ•ãƒ­ãƒ¼ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
  private taskDecomposer!: TaskDecomposer;
  private difficultyClassifier!: DifficultyClassifier;
  private qualityGate!: QualityGate;
  private collaborativeConfig!: CollaborativeConfig;
  private activeSessions: Map<string, CodingSession> = new Map();
  
  // IT Troubleshooting Services
  private logAnalysisService!: LogAnalysisService;
  private interactiveTroubleshooter!: InteractiveTroubleshooter;
  private advancedLogAnalyzer!: AdvancedLogAnalyzer;
  private safeExecutionManager!: SafeExecutionManager;
  
  // ã‚³ã‚¹ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
  private costManagement!: CostManagementSystem;
  
  // Capabilityç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
  private capabilityRegistry!: CapabilityRegistry;
  
  // OpenRouter Model Registry
  private openRouterRegistry: OpenRouterModelRegistry | undefined;
  
  // Model Alias Resolverï¼ˆå…¬å¼ã‚¨ã‚¤ãƒªã‚¢ã‚¹å¯¾å¿œï¼‰
  private aliasResolver!: ModelAliasResolver;
  
  // çŸ¥çš„åˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼ˆClaude Codeä¸»å°ï¼‰
  private queryAnalyzer!: ClaudeCodeQueryAnalyzer;
  private contextAwareAnalyzer!: ContextAwareQueryAnalyzer;
  private suitabilityAnalyzer!: ModelSuitabilityAnalyzer;
  
  // Redisçµ±åˆãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ï¼ˆUpstashå¯¾å¿œï¼‰
  private redisLogger!: RedisLogger | UpstashRedisLogger;
  
  // ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
  private conversationManager!: ConversationManager;
  
  // CLI Interface Manager removed - moved to ToolOrchestratorService

  constructor(configPath?: string) {
    console.log('[LLMOrchestrator] Initializing 5-Tier Hybrid LLM System with Collaborative Coding...');
    
    const configLoader = ConfigLoader.getInstance();
    this.config = configLoader.loadConfig(configPath);
    
    // ç’°å¢ƒå¤‰æ•°æ¤œè¨¼
    const envCheck = configLoader.validateEnvironmentVariables();
    if (!envCheck.valid) {
      console.warn('[LLMOrchestrator] Missing environment variables:', envCheck.missingVars);
    }
    if (envCheck.warnings.length > 0) {
      console.warn('[LLMOrchestrator] Environment warnings:', envCheck.warnings);
    }

    this.initializeAliasResolver();
    // CLI Manager initialization moved to ToolOrchestratorService
    this.initializeOpenRouterRegistry();
    this.initializeClients();
    this.initializeMetrics();
    this.initializeIntelligentAnalyzer();
    this.initializeCollaborativeComponents();
    this.initializeCapabilityRegistry();
    
    // Redisçµ±åˆãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    this.initializeRedisLogger();
    
    // ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    this.initializeConversationManager();
    
    // IT Troubleshooting Services åˆæœŸåŒ–
    this.initializeITTroubleshootingServices();
    
    // ã‚³ã‚¹ãƒˆç®¡ç†ã¯éåŒæœŸã§åˆæœŸåŒ–
    this.initializeCostManagement().catch(error => {
      console.warn('[LLMOrchestrator] Cost management initialization failed:', error);
    });
    
    // æ—¥æ¬¡ã‚³ã‚¹ãƒˆæ›´æ–°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é–‹å§‹
    this.startDailyCostScheduler();
    
    console.log('[LLMOrchestrator] âœ… System initialized successfully');
    this.printSystemSummary();
  }

  private initializeAliasResolver(): void {
    console.log('[LLMOrchestrator] Initializing Model Alias Resolver...');
    
    try {
      this.aliasResolver = ModelAliasResolver.getInstance();
      this.aliasResolver.loadConfig('./config/model-aliases.yaml');
      
      const stats = this.aliasResolver.getStats();
      console.log(`[LLMOrchestrator] âœ… Model Alias Resolver initialized: ${stats.providers} providers, ${stats.total_aliases} aliases, ${stats.official_alias_providers} official`);
      
    } catch (error) {
      console.error('[LLMOrchestrator] âŒ Failed to initialize Model Alias Resolver:', error);
      throw error;
    }
  }

  // initializeCLIManager method removed - moved to ToolOrchestratorService

  private initializeClients(): void {
    console.log('[LLMOrchestrator] Initializing API clients...');

    for (const [modelId, modelConfig] of Object.entries(this.config.models)) {
      try {
        let client: BaseLLMClient;
        
        // ã‚¨ã‚¤ãƒªã‚¢ã‚¹è§£æ±ºã§ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—
        const resolvedModelName = this.aliasResolver.resolveAlias(modelConfig.name);
        console.log(`[LLMOrchestrator] ğŸ”„ Resolved model alias: ${modelConfig.name} â†’ ${resolvedModelName}`);

        switch (modelConfig.provider) {
          case 'alibaba_cloud':
            // ç’°å¢ƒå¤‰æ•°ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯Mockã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
            if (!process.env.ALIBABA_ACCESS_KEY_ID || !process.env.ALIBABA_ACCESS_KEY_SECRET) {
              console.log(`[LLMOrchestrator] ğŸ”„ Using Mock Qwen3 Coder client (missing credentials)`);
              client = new MockQwenClient();
            } else {
              client = new QwenCoderAPIClient();
            }
            console.log(`[LLMOrchestrator] âœ… Qwen3 Coder client initialized (Tier ${modelConfig.tier})`);
            break;
          
          case 'openrouter':
            // OpenRouterçµŒç”±ã§Qwen3-Coderã‚’ä½¿ç”¨
            if (!process.env.OPENROUTER_API_KEY) {
              console.log(`[LLMOrchestrator] ğŸ”„ Using Mock Qwen3 Coder client (missing OpenRouter credentials)`);
              client = new MockQwenClient();
            } else {
              client = new OpenRouterAPIClient(resolvedModelName);
            }
            console.log(`[LLMOrchestrator] âœ… OpenRouter Qwen3 Coder client initialized (Tier ${modelConfig.tier})`);
            break;
          
          case 'google':
            client = new GeminiAPIClient(resolvedModelName);
            console.log(`[LLMOrchestrator] âœ… Gemini client initialized: ${resolvedModelName} (Tier ${modelConfig.tier})`);
            break;
          
          case 'anthropic':
            client = new AnthropicAPIClient(resolvedModelName);
            console.log(`[LLMOrchestrator] âœ… Anthropic client initialized: ${resolvedModelName} (${modelId})`);
            break;
          
          case 'openai':
            client = new OpenAIAPIClient(resolvedModelName);
            console.log(`[LLMOrchestrator] âœ… OpenAI client initialized: ${resolvedModelName} (${modelId})`);
            break;
          
          default:
            console.error(`[LLMOrchestrator] âŒ Unknown provider: ${modelConfig.provider} for model ${modelId}`);
            continue;
        }

        this.clients.set(modelId, client);

      } catch (error) {
        console.error(`[LLMOrchestrator] âŒ Failed to initialize client for ${modelId}:`, error);
      }
    }

    // OpenRouter Registry ã‹ã‚‰ã®å‹•çš„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    if (this.openRouterRegistry) {
      this.initializeOpenRouterClients();
    }

    console.log(`[LLMOrchestrator] Initialized ${this.clients.size} total clients`);
  }
  
  private initializeOpenRouterClients(): void {
    if (!this.openRouterRegistry) return;
    
    console.log('[LLMOrchestrator] Initializing OpenRouter dynamic clients...');
    
    try {
      const openRouterModels = this.openRouterRegistry.getAvailableModels();
      let dynamicClientCount = 0;
      
      for (const modelConfig of openRouterModels) {
        try {
          // æ—¢å­˜ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒãªã„å ´åˆã®ã¿ä½œæˆ
          if (!this.clients.has(modelConfig.id)) {
            const client = this.openRouterRegistry.getClient(modelConfig.id);
            if (client) {
              this.clients.set(modelConfig.id, client);
              dynamicClientCount++;
              console.log(`[LLMOrchestrator] âœ… OpenRouter client initialized: ${modelConfig.id} (Tier ${modelConfig.tier})`);
            }
          }
        } catch (error) {
          console.warn(`[LLMOrchestrator] âš ï¸ Failed to initialize OpenRouter client for ${modelConfig.id}:`, error);
        }
      }
      
      console.log(`[LLMOrchestrator] âœ… Initialized ${dynamicClientCount} OpenRouter dynamic clients`);
      
    } catch (error) {
      console.error('[LLMOrchestrator] âŒ Failed to initialize OpenRouter clients:', error);
    }
  }

  private initializeOpenRouterRegistry(): void {
    console.log('[LLMOrchestrator] Initializing OpenRouter Model Registry...');
    
    try {
      if (process.env.OPENROUTER_API_KEY) {
        this.openRouterRegistry = OpenRouterModelRegistry.getInstance();
        // è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        const configPath = './config/openrouter-models.yaml';
        this.openRouterRegistry.loadConfig(configPath);
        
        const modelCount = this.openRouterRegistry.getAvailableModels().length;
        console.log(`[LLMOrchestrator] âœ… OpenRouter Model Registry initialized with ${modelCount} models`);
      } else {
        console.warn('[LLMOrchestrator] âš ï¸ OpenRouter API key not found - dynamic models will be unavailable');
      }
    } catch (error) {
      console.error('[LLMOrchestrator] âŒ Failed to initialize OpenRouter Registry:', error);
      this.openRouterRegistry = undefined;
    }
  }

  private initializeMetrics(): void {
    this.metrics = {
      requests_per_tier: { 0: 0, 1: 0, 2: 0, 3: 0 },
      success_rate_per_tier: { 0: 0, 1: 0, 2: 0, 3: 0 },
      average_latency_per_tier: { 0: 0, 1: 0, 2: 0, 3: 0 },
      cost_per_tier: { 0: 0, 1: 0, 2: 0, 3: 0 },
      total_monthly_spend: 0,
      budget_utilization_percentage: 0,
      most_used_capabilities: [],
      error_distribution: {}
    };
  }

  private initializeIntelligentAnalyzer(): void {
    console.log('[LLMOrchestrator] ğŸ§  Initializing Claude Code-driven intelligent analysis system...');
    
    this.queryAnalyzer = new ClaudeCodeQueryAnalyzer();
    this.contextAwareAnalyzer = new ContextAwareQueryAnalyzer();
    this.suitabilityAnalyzer = new ModelSuitabilityAnalyzer();
    
    console.log('[LLMOrchestrator] âœ… Intelligent analysis system with context awareness initialized');
  }
  
  private initializeCollaborativeComponents(): void {
    console.log('[LLMOrchestrator] Initializing collaborative coding components...');
    
    // å”èª¿è¨­å®šã®åˆæœŸåŒ–
    this.collaborativeConfig = {
      difficultyThreshold: 0.6, // 60%ä»¥ä¸Šeasyåˆ¤å®šã§Qwen3ã«å§”ä»»
      maxRetries: 2,
      qcDepth: 'full',
      maxSubtasks: 10,
      enableParallelProcessing: true,
      autoEscalateToClaudeAfterRetries: true,
      qualityThresholds: {
        minScore: 70,
        requiresReview: 85
      }
    };
    
    // ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
    this.taskDecomposer = new TaskDecomposer(this.collaborativeConfig);
    this.difficultyClassifier = new DifficultyClassifier(this.collaborativeConfig);
    this.qualityGate = new QualityGate(this.collaborativeConfig);
    
    console.log('[LLMOrchestrator] âœ… Collaborative components initialized');
  }
  
  private initializeCapabilityRegistry(): void {
    console.log('[LLMOrchestrator] Initializing capability registry...');
    
    // CapabilityRegistryã®åˆæœŸåŒ–
    this.capabilityRegistry = new DefaultCapabilityRegistry();
    
    try {
      // OpenAI Assistant APIæ©Ÿèƒ½ã®è¨­å®šãƒ»ç™»éŒ²
      if (this.config.external_apis?.openai_assistant) {
        const assistantConfig = this.config.external_apis.openai_assistant;
        
        const assistantProviderConfig: AssistantConfig = {
          openai_api_key: process.env.OPENAI_API_KEY || '',
          model: assistantConfig.model || 'gpt-4o-mini',
          tools: (assistantConfig.tools || [
            { type: 'file_search' },
            { type: 'code_interpreter' }
          ]) as any,
          temperature: assistantConfig.temperature || 0.7,
          max_prompt_tokens: assistantConfig.max_prompt_tokens || 128000,
          max_completion_tokens: assistantConfig.max_completion_tokens || 4096,
          cost_per_1k_input_tokens: assistantConfig.cost_per_1k_input_tokens || 0.15,
          cost_per_1k_output_tokens: assistantConfig.cost_per_1k_output_tokens || 0.60
        };
        
        // OpenAI Assistant Providerã®ä½œæˆãƒ»ç™»éŒ²
        const assistantProvider = new OpenAIAssistantProvider(assistantProviderConfig);
        
        // éåŒæœŸã§åˆæœŸåŒ–ãƒ»ç™»éŒ²
        assistantProvider.initialize(assistantProviderConfig).then(() => {
          this.capabilityRegistry.register(assistantProvider);
          console.log('[LLMOrchestrator] âœ… OpenAI Assistant capability registered');
        }).catch(error => {
          console.warn('[LLMOrchestrator] âš ï¸ OpenAI Assistant initialization failed:', error);
        });
      }
      
      console.log('[LLMOrchestrator] âœ… Capability registry initialized');
      
    } catch (error) {
      console.error('[LLMOrchestrator] âŒ Capability registry initialization failed:', error);
    }
  }

  private initializeRedisLogger(): void {
    console.log('[LLMOrchestrator] Initializing Redis Logger...');
    
    try {
      // Upstash Redisè¨­å®šã‚’å„ªå…ˆçš„ã«ç¢ºèª
      const upstashUrl = process.env.UPSTASH_REDIS_REST_URL;
      const upstashToken = process.env.UPSTASH_REDIS_REST_TOKEN;
      
      if (upstashUrl && upstashToken) {
        console.log('[LLMOrchestrator] ğŸš€ Using Upstash Redis (cloud-native)');
        this.redisLogger = new UpstashRedisLogger();
        
        // éåŒæœŸæ¥ç¶š
        this.redisLogger.connect().catch(error => {
          console.warn('[LLMOrchestrator] Upstash Redis connection failed, falling back to local Redis:', error);
          this.fallbackToLocalRedis();
        });
        
        console.log('[LLMOrchestrator] âœ… Upstash Redis Logger initialized');
      } else {
        console.log('[LLMOrchestrator] ğŸ“ Using local Redis (fallback)');
        this.fallbackToLocalRedis();
      }
    } catch (error) {
      console.error('[LLMOrchestrator] âŒ Redis Logger initialization failed:', error);
      this.fallbackToLocalRedis();
    }
  }

  private fallbackToLocalRedis(): void {
    try {
      const redisUrl = process.env.REDIS_URL || 'redis://localhost:6379';
      this.redisLogger = new RedisLogger(redisUrl);
      
      this.redisLogger.connect().catch(error => {
        console.warn('[LLMOrchestrator] Local Redis connection failed, logging disabled:', error);
      });
      
      console.log('[LLMOrchestrator] âœ… Local Redis Logger initialized');
    } catch (error) {
      console.error('[LLMOrchestrator] âŒ Local Redis initialization failed:', error);
    }
  }

  private async initializeCostManagement(): Promise<void> {
    console.log('[LLMOrchestrator] Initializing precision cost management system...');
    
    try {
      // ã‚³ã‚¹ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
      this.costManagement = new PrecisionCostManagementSystem('./data/cost-management');
      
      // äºˆç®—è¨­å®šã®åˆæœŸåŒ–
      const budgetConfig = {
        monthly_budget_usd: this.config.cost_management?.monthly_budget_usd || 70.0,
        warning_threshold: this.config.cost_management?.cost_alerts?.warning_threshold || 0.8,
        critical_threshold: this.config.cost_management?.cost_alerts?.critical_threshold || 0.95,
        auto_pause_at_limit: false,
        max_request_cost_usd: 1.0, // å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æœ€å¤§ã‚³ã‚¹ãƒˆ
        max_session_cost_usd: 5.0, // ã‚»ãƒƒã‚·ãƒ§ãƒ³å˜ä½ã®æœ€å¤§ã‚³ã‚¹ãƒˆ
        budget_reset_day: 1,
        timezone: 'UTC'
      };
      
      await this.costManagement.initialize(budgetConfig);
      
      console.log('[LLMOrchestrator] âœ… Cost management system initialized');
      console.log(`[LLMOrchestrator] ğŸ’° Monthly budget: $${budgetConfig.monthly_budget_usd}`);
      console.log(`[LLMOrchestrator] ğŸš¨ Alert thresholds: ${(budgetConfig.warning_threshold * 100).toFixed(0)}% / ${(budgetConfig.critical_threshold * 100).toFixed(0)}%`);
      
    } catch (error) {
      console.error('[LLMOrchestrator] âŒ Failed to initialize cost management:', error);
      // åŸºæœ¬çš„ãªãƒ¢ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã§ç¶šè¡Œ
      console.log('[LLMOrchestrator] ğŸ”„ Continuing with basic cost tracking...');
    }
  }

  private initializeConversationManager(): void {
    console.log('[LLMOrchestrator] ğŸ’¬ Initializing Conversation Manager...');
    
    try {
      this.conversationManager = new ConversationManager(this.redisLogger);
      console.log('[LLMOrchestrator] âœ… Conversation Manager initialized with Redis backend');
    } catch (error) {
      console.error('[LLMOrchestrator] âŒ Failed to initialize Conversation Manager:', error);
      throw error;
    }
  }

  private initializeITTroubleshootingServices(): void {
    console.log('[LLMOrchestrator] ğŸ”§ Initializing IT Troubleshooting Services...');
    
    try {
      this.logAnalysisService = new LogAnalysisService();
      this.interactiveTroubleshooter = new InteractiveTroubleshooter();
      this.advancedLogAnalyzer = new AdvancedLogAnalyzer();
      this.safeExecutionManager = new SafeExecutionManager();
      
      console.log('[LLMOrchestrator] âœ… IT Troubleshooting Services initialized:');
      console.log('   - Log Analysis Service');
      console.log('   - Interactive Troubleshooter');
      console.log('   - Advanced Log Analyzer');
      console.log('   - Safe Execution Manager');
    } catch (error) {
      console.error('[LLMOrchestrator] âŒ Failed to initialize IT Troubleshooting Services:', error);
      throw error;
    }
  }

  private printSystemSummary(): void {
    console.log('\nğŸš€ === 5-Tier Hybrid LLM System Summary ===');
    
    const allModels = this.getAvailableModels();
    const modelsByTier: Record<number, ModelConfig[]> = {};
    
    // ãƒ¢ãƒ‡ãƒ«ã‚’Tierã”ã¨ã«åˆ†é¡
    for (const model of allModels) {
      if (!modelsByTier[model.tier]) {
        modelsByTier[model.tier] = [];
      }
      modelsByTier[model.tier].push(model);
    }
    
    // Tierã”ã¨ã«è¡¨ç¤º
    for (let tier = 0; tier <= 3; tier++) {
      const tierModels = modelsByTier[tier] || [];
      if (tierModels.length > 0) {
        console.log(`\nTier ${tier}: ${tierModels.length} models`);
        tierModels.forEach(model => {
          const status = this.clients.has(model.id) ? 'âœ…' : 'âŒ';
          const source = model.api_client === 'UniversalOpenRouterClient' ? '(OpenRouter)' : '(Direct)';
          console.log(`  ${status} ${model.id} ${source} - ${model.capabilities.join(', ')}`);
        });
      }
    }
    
    if (this.openRouterRegistry) {
      const orModels = this.openRouterRegistry.getAvailableModels();
      console.log(`\nğŸŒ OpenRouter Models: ${orModels.length} available`);
    }
    
    console.log(`\nğŸ’° Monthly Budget: $${this.config.cost_management.monthly_budget_usd}`);
    console.log(`ğŸ”„ Collaboration: Cascade=${this.config.collaboration.cascade_enabled}, Refinement=${this.config.collaboration.refinement_enabled}`);
    console.log(`ğŸ¤ Collaborative Coding: Enabled with ${this.collaborativeConfig.maxSubtasks} max subtasks`);
    console.log('==========================================\n');
  }

  async process(request: LLMRequest): Promise<LLMResponse> {
    console.log(`\nğŸš¨ğŸš¨ğŸš¨ [LLMOrchestrator] EMERGENCY TEST - Process method called! ğŸš¨ğŸš¨ğŸš¨`);
    console.log(`\n[LLMOrchestrator] Processing request: "${request.prompt.substring(0, 100)}${request.prompt.length > 100 ? '...' : ''}"`);
    console.log(`[LLMOrchestrator] Task type: ${request.task_type || 'auto'}`);
    console.log(`[LLMOrchestrator] ***** PROCESS METHOD ENTRY POINT *****`);
    
    this.requestCount++;
    const startTime = Date.now();
    
    // ğŸ†• ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å‡¦ç†
    let conversationContext = request.context;
    let conversationId: string | undefined;
    
    if (request.user_metadata?.session_id) {
      conversationId = request.user_metadata.session_id;
      console.log(`[LLMOrchestrator] ğŸ’¬ Using existing conversation: ${conversationId}`);
      
      // æ—¢å­˜ä¼šè©±ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
      if (!conversationContext) {
        conversationContext = await this.conversationManager.buildConversationContext(conversationId);
        console.log(`[LLMOrchestrator] ğŸ“– Built context with ${conversationContext?.turn_count || 0} turns`);
      }
    }

    // Vector Storageç­‰ã®æ–°æ©Ÿèƒ½ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ãƒã‚§ãƒƒã‚¯
    const isCapabilityReq = this.isCapabilityRequest(request);
    console.log(`[LLMOrchestrator] isCapabilityRequest: ${isCapabilityReq}`);
    if (isCapabilityReq) {
      console.log(`[LLMOrchestrator] Routing to capability provider`);
      return this.processWithCapabilityProvider(request);
    }

    // å”èª¿ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    const shouldUseCollaborativeCoding = this.shouldUseCollaborativeCoding(request);
    console.log(`[LLMOrchestrator] shouldUseCollaborativeCoding: ${shouldUseCollaborativeCoding}`);
    
    if (shouldUseCollaborativeCoding) {
      console.log(`[LLMOrchestrator] ğŸ¤ Routing to collaborative coding pipeline`);
      
      const decompositionRequest: DecompositionRequest = {
        originalPrompt: request.prompt,
        targetLanguage: this.extractTargetLanguage(request.prompt),
        complexityPreference: 'balanced',
        maxSubtasks: this.collaborativeConfig.maxSubtasks,
        context: typeof request.context === 'string' ? request.context : undefined
      };
      
      try {
        const session = await this.processCollaborativeCoding(decompositionRequest);
        
        // CodingSessionã‚’LLMResponseã«å¤‰æ›
        return this.convertSessionToResponse(session, startTime);
        
      } catch (error) {
        console.warn(`[LLMOrchestrator] Collaborative coding failed, falling back to standard processing:`, error);
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¦é€šå¸¸å‡¦ç†ã‚’å®Ÿè¡Œ
      }
    }

    try {
      // 1. ğŸ†• ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè€ƒæ…®å‹ã®çŸ¥çš„ã‚¿ã‚¹ã‚¯åˆ†æ
      console.log(`[LLMOrchestrator] ğŸ” DETAILED DEBUG - Request task_type: ${request.task_type}`);
      console.log(`[LLMOrchestrator] ğŸ” DETAILED DEBUG - Request prompt (first 50 chars): ${request.prompt.substring(0, 50)}`);
      
      let { taskType, analysis } = await this.classifyTaskIntelligently(request);
      
      // ğŸ†• ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè€ƒæ…®åˆ†æã‚’å®Ÿè¡Œ
      if (conversationContext) {
        console.log(`[LLMOrchestrator] ğŸ§  Performing context-aware analysis...`);
        analysis = await this.contextAwareAnalyzer.analyzeWithContext(request, conversationContext);
        
        // è¤‡é›‘åº¦ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—å†åˆ†é¡
        if (analysis.context_factors?.complexity_escalation && analysis.context_factors.complexity_escalation > 1.5) {
          const escalatedTaskType = this.escalateTaskType(taskType);
          if (escalatedTaskType !== taskType) {
            console.log(`[LLMOrchestrator] â¬†ï¸ Task type escalated: ${taskType} â†’ ${escalatedTaskType}`);
            taskType = escalatedTaskType;
          }
        }
      }
      
      console.log(`[LLMOrchestrator] ğŸ¯ Task intelligently classified as: ${taskType}`);

      // 2. çŸ¥çš„ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆåˆ†æçµæœã‚’æ´»ç”¨ï¼‰
      console.log(`[LLMOrchestrator] ğŸ”„ About to call intelligent model selection with analysis...`);
      const selectedModel = await this.selectBestModelIntelligently(request, taskType, analysis);
      console.log(`[LLMOrchestrator] âœ… Selected model: ${selectedModel.id} (Tier ${selectedModel.tier})`);

      // 3. ã‚¯ã‚¨ãƒªåˆ†æãƒˆãƒ¬ãƒ¼ã‚¹ãƒ­ã‚°ï¼ˆRedisï¼‰
      const requestId = `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
      await this.logQueryAnalysis(requestId, request, analysis, selectedModel, taskType);

      // 4. äºˆç®—ãƒã‚§ãƒƒã‚¯
      if (!this.checkBudget(selectedModel)) {
        console.warn('[LLMOrchestrator] Budget exceeded, attempting fallback...');
        const fallbackModel = this.selectFallbackModel(selectedModel);
        if (!fallbackModel) {
          throw new Error('Budget exceeded and no fallback available');
        }
        return this.executeRequest(request, fallbackModel);
      }

      // 5. ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
      const response = await this.executeRequest(request, selectedModel);

      // 6. å“è³ªè©•ä¾¡ã¨ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰åˆ¤å®š
      if (this.shouldCascade(response, selectedModel)) {
        console.log('[LLMOrchestrator] Quality threshold not met, cascading to higher tier...');
        const cascadedResponse = await this.cascadeToHigherTier(request, selectedModel, response);
        
        // ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰å¾Œã‚‚ä¼šè©±å±¥æ­´ã«è¨˜éŒ²
        if (conversationId && cascadedResponse.success) {
          try {
            await this.conversationManager.addTurn(conversationId, request, cascadedResponse);
            console.log(`[LLMOrchestrator] ğŸ’¾ Cascaded turn saved to conversation ${conversationId}`);
          } catch (convError) {
            console.warn(`[LLMOrchestrator] âš ï¸ Failed to save cascaded conversation turn:`, convError);
          }
        }
        
        return cascadedResponse;
      }

      // 7. æ´—ç·´åŒ–åˆ¤å®š
      if (this.shouldRefine(response, selectedModel)) {
        console.log('[LLMOrchestrator] Applying refinement with higher tier model...');
        const refinedResponse = await this.refineWithHigherTier(request, response);
        
        // æ´—ç·´åŒ–å¾Œã‚‚ä¼šè©±å±¥æ­´ã«è¨˜éŒ²
        if (conversationId && refinedResponse.success) {
          try {
            await this.conversationManager.addTurn(conversationId, request, refinedResponse);
            console.log(`[LLMOrchestrator] ğŸ’¾ Refined turn saved to conversation ${conversationId}`);
          } catch (convError) {
            console.warn(`[LLMOrchestrator] âš ï¸ Failed to save refined conversation turn:`, convError);
          }
        }
        
        return refinedResponse;
      }

      // ğŸ†• ä¼šè©±å±¥æ­´ã«è¨˜éŒ²ï¼ˆæˆåŠŸæ™‚ã®ã¿ï¼‰
      if (conversationId && response.success) {
        try {
          await this.conversationManager.addTurn(conversationId, request, response);
          console.log(`[LLMOrchestrator] ğŸ’¾ Turn saved to conversation ${conversationId}`);
        } catch (convError) {
          console.warn(`[LLMOrchestrator] âš ï¸ Failed to save conversation turn:`, convError);
        }
      }
      
      console.log(`[LLMOrchestrator] âœ… Request completed successfully with ${selectedModel.id}`);
      return response;

    } catch (error) {
      console.error('[LLMOrchestrator] âŒ Request failed:', error);
      
      const errorResponse: LLMResponse = {
        success: false,
        model_used: 'orchestrator_error',
        tier_used: -1,
        error: {
          code: 'ORCHESTRATOR_ERROR',
          message: error instanceof Error ? error.message : 'Unknown orchestrator error'
        },
        metadata: {
          model_id: 'orchestrator_error',
          provider: 'system',
          tokens_used: { input: 0, output: 0, total: 0 },
          generated_at: new Date().toISOString(),
          tier_used: -1,
          processing_time_ms: 0,
          estimated_complexity: 0
        },
        cost_info: {
          total_cost_usd: 0,
          input_cost_usd: 0,
          output_cost_usd: 0
        },
        performance_info: {
          latency_ms: Date.now() - startTime,
          processing_time_ms: Date.now() - startTime,
          fallback_used: false
        }
      };

      return errorResponse;
    }
  }

  /**
   * Claude Codeä¸»å°ã®çŸ¥çš„ã‚¿ã‚¹ã‚¯åˆ†é¡
   * å¾“æ¥ã®å˜ç´”ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‹ã‚‰ã€å¤šæ¬¡å…ƒçš„æ„å›³ç†è§£ã¸é€²åŒ–
   */
  private async classifyTaskIntelligently(request: LLMRequest): Promise<{taskType: TaskType, analysis: QueryAnalysis}> {
    // ãƒ¦ãƒ¼ã‚¶ãƒ¼æ˜ç¤ºçš„æŒ‡å®šãŒæœ€å„ªå…ˆ
    if (request.task_type && request.task_type !== 'auto') {
      console.log(`[LLMOrchestrator] ğŸ‘¤ User specified task type: ${request.task_type}`);
      
      // æ˜ç¤ºçš„æŒ‡å®šã§ã‚‚åˆ†æã‚’å®Ÿè¡Œï¼ˆå“è³ªå‘ä¸Šã®ãŸã‚ï¼‰
      const analysis = await this.queryAnalyzer.analyzeQuery(request.prompt, { userSpecified: true });
      return { taskType: request.task_type, analysis };
    }

    console.log(`[LLMOrchestrator] ğŸ§  Performing Claude Code intelligent analysis...`);
    
    // Claude Codeã«ã‚ˆã‚‹æ·±å±¤åˆ†æ
    const analysis = await this.queryAnalyzer.analyzeQuery(request.prompt, request);
    
    // åˆ†æçµæœã«åŸºã¥ãã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—æ±ºå®š
    const taskType = this.determineTaskTypeFromAnalysis(analysis, request.prompt);
    
    console.log(`[LLMOrchestrator] ğŸ“Š Intelligence Analysis Result:`);
    console.log(`  ğŸ¯ Task Type: ${taskType}`);
    console.log(`  ğŸ”¬ Complexity: ${analysis.complexity}`);
    console.log(`  ğŸ·ï¸  Domain: ${analysis.domain.join(', ')}`);
    console.log(`  ğŸ§® Required Capabilities: ${analysis.requiredCapabilities.join(', ')}`);
    console.log(`  âš¡ Priority Balance: Accuracy=${(analysis.priorityBalance.accuracy*100).toFixed(0)}% Speed=${(analysis.priorityBalance.speed*100).toFixed(0)}% Cost=${(analysis.priorityBalance.cost*100).toFixed(0)}%`);
    console.log(`  ğŸ¨ Creativity Level: ${analysis.creativityLevel}`);
    console.log(`  ğŸ’­ Reasoning Depth: ${analysis.reasoningDepth}`);
    console.log(`  â±ï¸  Est. Processing: ${analysis.estimatedProcessingTime.toFixed(1)}s`);
    console.log(`  ğŸª Confidence: ${(analysis.confidenceScore*100).toFixed(1)}%`);
    
    return { taskType, analysis };
  }

  /**
   * åˆ†æçµæœã‹ã‚‰ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®šã™ã‚‹çŸ¥çš„ãƒ­ã‚¸ãƒƒã‚¯
   */
  private determineTaskTypeFromAnalysis(analysis: QueryAnalysis, prompt: string): TaskType {
    // GPT-5é©ç”¨ã‚’ç©æ¥µçš„ã«åˆ¤å®šï¼ˆå°‚é–€æ€§ãƒ»è¤‡é›‘åº¦ãƒ»å“è³ªè¦æ±‚ã®ç·åˆè©•ä¾¡ï¼‰
    const gpt5Indicators = [
      analysis.complexity === 'expert',
      analysis.reasoningDepth === 'deep',
      analysis.qualityRequirement === 'exceptional',
      analysis.creativityLevel === 'innovative',
      analysis.domain.length > 2, // è¤‡æ•°å°‚é–€åˆ†é‡ã«ã¾ãŸãŒã‚‹
      analysis.priorityBalance.accuracy > 0.8, // æ¥µé«˜ç²¾åº¦è¦æ±‚
      prompt.toLowerCase().includes('æˆ¦ç•¥') || prompt.toLowerCase().includes('strategic'),
      prompt.toLowerCase().includes('é‡è¦') || prompt.toLowerCase().includes('critical'),
      prompt.toLowerCase().includes('æœ€é«˜') || prompt.toLowerCase().includes('ultimate'),
      analysis.estimatedProcessingTime > 20 // é•·æ™‚é–“å‡¦ç†äºˆæ¸¬
    ];
    
    const gpt5Score = gpt5Indicators.filter(Boolean).length;
    
    // GPT-5é©ç”¨æ¡ä»¶ã‚’ç·©å’Œï¼ˆ2å€‹ä»¥ä¸Šã®æŒ‡æ¨™ã§é©ç”¨ï¼‰
    if (gpt5Score >= 2) {
      console.log(`[QueryAnalyzer] ğŸš€ GPT-5 selection criteria met (${gpt5Score}/10 indicators)`);
      return 'critical'; // GPT-5 Tier 4ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
    }

    // è¤‡é›‘åº¦ãƒ™ãƒ¼ã‚¹ã®åŸºæœ¬åˆ¤å®š
    if (analysis.complexity === 'expert' || analysis.complexity === 'complex') {
      // å°‚é–€æ€§ãŒé«˜ã„å ´åˆã®è©³ç´°åˆ¤å®š
      if (analysis.requiredCapabilities.includes('coding') || analysis.domain.includes('technology')) {
        return 'coding';
      }
      
      if (analysis.intentCategory === 'analysis' || analysis.reasoningDepth === 'deep') {
        return 'complex_analysis';
      }
      
      if (analysis.qualityRequirement === 'exceptional' || analysis.priorityBalance.accuracy > 0.7) {
        return 'premium';
      }
      
      return 'complex_analysis'; // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¤‡é›‘ã‚¿ã‚¹ã‚¯
    }

    // ä¸­ç¨‹åº¦è¤‡é›‘åº¦ã®å ´åˆ
    if (analysis.complexity === 'moderate') {
      if (analysis.requiredCapabilities.includes('coding')) {
        return 'coding';
      }
      
      if (analysis.intentCategory === 'analysis' || analysis.domain.length > 1) {
        return 'complex_analysis';
      }
      
      return 'general';
    }

    // å‰µé€ æ€§ãƒ»å“è³ªè¦æ±‚ã«ã‚ˆã‚‹åˆ¤å®š
    if (analysis.creativityLevel === 'creative' || analysis.creativityLevel === 'innovative') {
      if (analysis.qualityRequirement === 'exceptional') {
        return 'premium';
      }
      return 'complex_analysis';
    }

    // æ„å›³ã‚«ãƒ†ã‚´ãƒªã«ã‚ˆã‚‹åˆ¤å®š
    if (analysis.intentCategory === 'decision' && analysis.qualityRequirement === 'high') {
      return 'premium';
    }

    // å¾“æ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰
    const legacyTaskType = this.classifyTaskLegacy(prompt);
    if (legacyTaskType !== 'general') {
      console.log(`[LLMOrchestrator] ğŸ“ Legacy keyword match: ${legacyTaskType}`);
      return legacyTaskType;
    }

    // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return 'general';
  }

  /**
   * å¾“æ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹åˆ†é¡ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
   */
  private classifyTaskLegacy(prompt: string): TaskType {
    const promptLower = prompt.toLowerCase();

    for (const [taskType, rules] of Object.entries(this.config.routing.task_classification)) {
      const matchCount = rules.keywords.filter(keyword => 
        promptLower.includes(keyword.toLowerCase())
      ).length;
      
      if (matchCount > 0) {
        return taskType as TaskType;
      }
    }

    return 'general';
  }

  /**
   * Claude Codeä¸»å°ã®çŸ¥çš„ãƒ¢ãƒ‡ãƒ«é¸æŠ
   * åˆ†æçµæœã‚’æ´»ç”¨ã—ãŸå‹•çš„é©æ€§è©•ä¾¡ã«åŸºã¥ãæœ€é©ãƒ¢ãƒ‡ãƒ«æ±ºå®š
   */
  private async selectBestModelIntelligently(
    request: LLMRequest, 
    taskType: TaskType, 
    analysis: QueryAnalysis
  ): Promise<ModelConfig> {
    console.log(`[LLMOrchestrator] ğŸ§  Performing intelligent model selection...`);

    // ğŸš« Gemini Flashä½ç²¾åº¦å¯¾ç­–: å¼·åˆ¶Tieræ˜‡æ ¼æ¡ä»¶
    const forcedEscalation = this.evaluateForcedTierEscalation(taskType, analysis, request);
    
    // åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ã®å–å¾—ï¼ˆæœ€å°Tieråˆ¶é™é©ç”¨ï¼‰
    const availableModels = Object.values(this.config.models)
      .filter(model => this.clients.has(model.id))
      .filter(model => model.tier >= forcedEscalation.minTier);

    console.log(`[LLMOrchestrator] ğŸ“Š Available models (Tier ${forcedEscalation.minTier}+): ${availableModels.map(m => `${m.id}(T${m.tier})`).join(', ')}`);

    // å¼·åˆ¶ãƒ¢ãƒ‡ãƒ«æŒ‡å®šãŒã‚ã‚‹å ´åˆ
    if (forcedEscalation.forcedModel) {
      const forcedModel = availableModels.find(m => m.id === forcedEscalation.forcedModel);
      if (forcedModel) {
        console.log(`[LLMOrchestrator] ğŸ”¥ FORCED ESCALATION: ${forcedModel.id} (${forcedEscalation.reasoning})`);
        return forcedModel;
      }
    }

    // å‹•çš„ãƒ¢ãƒ‡ãƒ«é©æ€§è©•ä¾¡
    const suitabilityScores = this.suitabilityAnalyzer.evaluateModelForTask(
      analysis,
      availableModels,
      new Map() // ãƒ¢ãƒ‡ãƒ«èƒ½åŠ›ãƒãƒƒãƒ—
    );

    // ğŸ†• Gemini FlashæŠ‘åˆ¶ãƒ­ã‚¸ãƒƒã‚¯
    if (forcedEscalation.suppressLowTier) {
      suitabilityScores.forEach(score => {
        const model = availableModels.find(m => m.id === score.modelId);
        if (model && (model.id.includes('flash') || model.tier < 2)) {
          score.suitabilityScore *= 0.2; // Flashç³»ãƒ»ä½Tierã¯å¤§å¹…æ¸›ç‚¹
          console.log(`[LLMOrchestrator] â¬‡ï¸ Suppressing ${model.id} due to precision requirements`);
        }
      });
    }

    // é©æ€§ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
    suitabilityScores.sort((a, b) => b.suitabilityScore - a.suitabilityScore);

    console.log(`[LLMOrchestrator] ğŸ† Model Suitability Rankings:`);
    suitabilityScores.forEach((score, index) => {
      console.log(`  ${index + 1}. ${score.modelId}: ${(score.suitabilityScore * 100).toFixed(1)}% (${score.reasoning})`);
      if (score.strengths.length > 0) {
        console.log(`     âœ… Strengths: ${score.strengths.join(', ')}`);
      }
      if (score.weaknesses.length > 0) {
        console.log(`     âš ï¸  Weaknesses: ${score.weaknesses.join(', ')}`);
      }
    });

    // æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠ
    const bestModel = availableModels.find(model => 
      model.id === suitabilityScores[0].modelId
    );

    if (!bestModel) {
      console.log(`[LLMOrchestrator] âš ï¸ Intelligent selection failed, falling back to legacy method`);
      return this.selectBestModelLegacy(request, taskType);
    }

    console.log(`[LLMOrchestrator] ğŸ¯ Intelligently selected: ${bestModel.id} (Tier ${bestModel.tier}) with ${(suitabilityScores[0].suitabilityScore * 100).toFixed(1)}% suitability`);

    return bestModel;
  }

  /**
   * å¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
   */
  private selectBestModelLegacy(request: LLMRequest, taskType: TaskType): ModelConfig {
    console.log(`[LLMOrchestrator] ===== ENTERING selectBestModel =====`);
    console.log(`[LLMOrchestrator] ğŸ“ Input task type: ${taskType}`);
    console.log(`[LLMOrchestrator] ğŸ‘¤ User preferred tier: ${request.preferred_tier}`);

    // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç‰¹å®šã®Tierã‚’æŒ‡å®šã—ã¦ã„ã‚‹å ´åˆ
    if (request.preferred_tier !== undefined) {
      const tierModels = Object.values(this.config.models)
        .filter(model => model.tier === request.preferred_tier)
        .filter(modelId => this.clients.has(modelId.id));
      
      console.log(`[LLMOrchestrator] User specified tier ${request.preferred_tier}, found ${tierModels.length} available models`);
      if (tierModels.length > 0) {
        // è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆã¯èƒ½åŠ›ãƒ™ãƒ¼ã‚¹ã§é¸æŠ
        if (tierModels.length === 1) {
          console.log(`[LLMOrchestrator] Selected: ${tierModels[0].id} (only option in tier)`);
          return tierModels[0];
        } else {
          const selectedModel = this.selectModelByCapabilities(tierModels, taskType, request.prompt);
          console.log(`[LLMOrchestrator] Selected: ${selectedModel.id} (user tier + capabilities)`);
          return selectedModel;
        }
      }
    }

    // ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ãŸæ¨å¥¨Tierã‚’å–å¾—
    console.log(`[LLMOrchestrator] ğŸ” Looking up task rules for: '${taskType}'`);
    console.log(`[LLMOrchestrator] ğŸ” Available task types: ${Object.keys(this.config.routing.task_classification).join(', ')}`);
    const taskRules = this.config.routing.task_classification[taskType];
    const defaultTier = this.config.routing.default_tier;
    const preferredTier = taskRules?.preferred_tier !== undefined ? taskRules.preferred_tier : defaultTier;

    console.log(`[LLMOrchestrator] ğŸ“‹ Task rules for '${taskType}':`, JSON.stringify(taskRules, null, 2));
    console.log(`[LLMOrchestrator] ğŸ¯ Default tier: ${defaultTier}`);
    console.log(`[LLMOrchestrator] ğŸ¯ Final preferred tier: ${preferredTier}`);

    // æ¨å¥¨Tierã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    let candidateModels = Object.values(this.config.models)
      .filter(model => model.tier === preferredTier)
      .filter(model => this.clients.has(model.id));

    console.log(`[LLMOrchestrator] Tier ${preferredTier} models: ${candidateModels.map(m => m.id).join(', ')}`);
    console.log(`[LLMOrchestrator] Available clients: ${Array.from(this.clients.keys()).join(', ')}`);

    // æ¨å¥¨Tierã«ãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if (candidateModels.length === 0) {
      console.log(`[LLMOrchestrator] No models found in preferred tier ${preferredTier}, falling back...`);
      candidateModels = Object.values(this.config.models)
        .filter(model => this.clients.has(model.id))
        .sort((a, b) => a.tier - b.tier); // Tierã®ä½ã„é †ï¼ˆã‚³ã‚¹ãƒˆåŠ¹ç‡é‡è¦–ï¼‰
      
      console.log(`[LLMOrchestrator] Fallback candidates: ${candidateModels.map(m => `${m.id}(T${m.tier})`).join(', ')}`);
    }

    if (candidateModels.length === 0) {
      throw new Error('No available models found');
    }

    // ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    const selectedModel = this.selectModelByCapabilities(candidateModels, taskType, request.prompt);
    console.log(`[LLMOrchestrator] Final selection: ${selectedModel.id} (Tier ${selectedModel.tier})`);
    return selectedModel;
  }

  private selectModelByCapabilities(models: ModelConfig[], taskType: TaskType, prompt: string): ModelConfig {
    if (models.length === 1) {
      return models[0];
    }

    // ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã¨capabilityã®å¯¾å¿œãƒãƒƒãƒ”ãƒ³ã‚°
    const taskCapabilityMap: Record<string, string[]> = {
      'complex_analysis': ['complex_analysis', 'advanced_reasoning', 'architectural_design'],
      'coding': ['coding', 'code_generation', 'debugging', 'code_review'],
      'general': ['general_inquiry', 'fast_processing', 'validation'],
      'premium': ['premium_analysis', 'high_quality_generation', 'strategic_planning'],
      'critical': ['critical_decisions', 'ultimate_reasoning', 'strategic_planning']
    };

    const relevantCapabilities = taskCapabilityMap[taskType] || [];

    // å„ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
    const modelScores = models.map(model => {
      let score = 0;

      // Capability ãƒãƒƒãƒãƒ³ã‚°
      if (model.capabilities) {
        const matches = relevantCapabilities.filter(cap => 
          model.capabilities!.includes(cap as any)
        ).length;
        score += matches * 10; // Capability ãƒãƒƒãƒã”ã¨ã«10ãƒã‚¤ãƒ³ãƒˆ
      }

      // Priority keywords ãƒãƒƒãƒãƒ³ã‚°
      if (model.priority_keywords && prompt) {
        const promptLower = prompt.toLowerCase();
        const keywordMatches = model.priority_keywords.filter(keyword =>
          promptLower.includes(keyword.toLowerCase())
        ).length;
        score += keywordMatches * 5; // ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã”ã¨ã«5ãƒã‚¤ãƒ³ãƒˆ
      }

      // ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã«ã‚ˆã‚‹è»½å¾®ãªèª¿æ•´ï¼ˆä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒè‹¥å¹²æœ‰åˆ©ï¼‰
      score += Math.max(0, 10 - (model.latency_ms || 1000) / 100);

      return {
        model,
        score
      };
    });

    // ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆã—ã¦æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    modelScores.sort((a, b) => b.score - a.score);

    console.log(`[LLMOrchestrator] Model selection scores for task '${taskType}':`);
    modelScores.forEach(({ model, score }) => {
      console.log(`  ${model.id}: ${score.toFixed(1)} points`);
    });

    return modelScores[0].model;
  }

  private selectFallbackModel(currentModel: ModelConfig): ModelConfig | null {
    // ã‚ˆã‚Šä½ã„Tierã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
    const fallbackModels = Object.values(this.config.models)
      .filter(model => model.tier < currentModel.tier)
      .filter(model => this.clients.has(model.id))
      .sort((a, b) => b.tier - a.tier); // é«˜ã„Tierã‹ã‚‰é †ã«

    return fallbackModels.length > 0 ? fallbackModels[0] : null;
  }

  private async executeRequest(request: LLMRequest, modelConfig: ModelConfig, sessionId?: string): Promise<LLMResponse> {
    console.log(`[LLMOrchestrator] ğŸ¯ executeRequest DEBUG - modelConfig.id: ${modelConfig.id}`);
    console.log(`[LLMOrchestrator] ğŸ¯ executeRequest DEBUG - available clients: ${Array.from(this.clients.keys()).join(', ')}`);
    
    const client = this.clients.get(modelConfig.id);
    console.log(`[LLMOrchestrator] ğŸ¯ executeRequest DEBUG - found client: ${client ? 'YES' : 'NO'}`);
    
    if (!client) {
      throw new Error(`Client not available for model: ${modelConfig.id}`);
    }

    // ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰ã®ã‚³ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯
    const inputTokens = Math.ceil(request.prompt.length / 4); // æ¦‚ç®—
    
    // ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã¨å…¥åŠ›é•·ã«åŸºã¥ãç¾å®Ÿçš„ãªå‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ¨å®š
    let estimatedOutputTokens: number;
    if (request.task_type === 'coding') {
      // ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã¯å…¥åŠ›ã®2-3å€ç¨‹åº¦
      estimatedOutputTokens = Math.min(inputTokens * 2.5, 800);
    } else if (request.task_type === 'complex_analysis') {
      // åˆ†æã‚¿ã‚¹ã‚¯ã¯å…¥åŠ›ã®1.5-2å€ç¨‹åº¦  
      estimatedOutputTokens = Math.min(inputTokens * 1.8, 600);
    } else if (request.prompt.length > 1000) {
      // é•·ã„å…¥åŠ›ã®å ´åˆã¯æ¯”ä¾‹çš„ã«å¢—åŠ 
      estimatedOutputTokens = Math.min(inputTokens * 1.2, 500);
    } else {
      // çŸ­ã„å…¥åŠ›ã®å ´åˆã¯å›ºå®šçš„ãªæ¨å®š
      estimatedOutputTokens = Math.min(inputTokens * 1.5 + 50, 300);
    }
    
    const estimatedTokens: Partial<TokenUsage> = {
      input: inputTokens,
      output: Math.ceil(estimatedOutputTokens)
    };

    let preCheckResult = null;
    if (this.costManagement) {
      preCheckResult = await this.costManagement.preRequestCheck(modelConfig.id, estimatedTokens);
      
      if (!preCheckResult.approved) {
        console.error(`[LLMOrchestrator] âŒ Request rejected by cost management: ${preCheckResult.reason}`);
        return {
          success: false,
          model_used: modelConfig.id,
          tier_used: modelConfig.tier,
          error: {
            code: 'COST_LIMIT_EXCEEDED',
            message: preCheckResult.reason || 'Request rejected due to cost constraints',
            provider_error: null,
            retry_count: 0
          },
          metadata: {
            model_id: modelConfig.id,
            provider: modelConfig.provider,
            tokens_used: { input: 0, output: 0, total: 0 },
            tier_used: modelConfig.tier,
            estimated_complexity: 1,
            processing_time_ms: 0,
            generated_at: new Date().toISOString()
          },
          cost_info: {
            total_cost_usd: 0,
            input_cost_usd: 0,
            output_cost_usd: 0
          },
          performance_info: {
            latency_ms: 0,
            processing_time_ms: 0,
            fallback_used: false
          }
        };
      }

      if (preCheckResult.warnings.length > 0) {
        console.warn(`[LLMOrchestrator] âš ï¸ Cost warnings: ${preCheckResult.warnings.join(', ')}`);
      }
    }

    console.log(`[LLMOrchestrator] ğŸš€ Executing request with ${modelConfig.id}...`);
    if (preCheckResult) {
      console.log(`[LLMOrchestrator] ğŸ’° Estimated cost: $${preCheckResult.estimated_cost.total_cost_usd.toFixed(6)}`);
    }
    
    const startTime = Date.now();
    let response: LLMResponse;
    let success = false;
    let error: Error | undefined;

    try {
      response = await client.generate(request.prompt, {
        max_tokens: modelConfig.max_tokens,
        temperature: 0.7,
      });
      success = response.success;
    } catch (err) {
      error = err as Error;
      response = {
        success: false,
        model_used: modelConfig.id,
        tier_used: modelConfig.tier,
        error: {
          code: 'GENERATION_ERROR',
          message: error.message,
          provider_error: error,
          retry_count: 0
        },
        metadata: {
          model_id: modelConfig.id,
          provider: modelConfig.provider,
          tokens_used: { input: estimatedTokens.input || 0, output: 0, total: estimatedTokens.input || 0 },
          tier_used: modelConfig.tier,
          estimated_complexity: 1,
          processing_time_ms: Date.now() - startTime,
          generated_at: new Date().toISOString()
        },
        cost_info: {
          total_cost_usd: 0,
          input_cost_usd: 0,
          output_cost_usd: 0
        },
        performance_info: {
          latency_ms: Date.now() - startTime,
          processing_time_ms: Date.now() - startTime,
          fallback_used: false
        }
      };
    }

    const latency = Date.now() - startTime;

    // ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¾Œã®ã‚³ã‚¹ãƒˆå‡¦ç†
    if (this.costManagement && sessionId) {
      const actualTokens: TokenUsage = response.metadata?.tokens_used || {
        input: estimatedTokens.input || 0,
        output: success ? (estimatedTokens.output || 0) : 0,
        total: (estimatedTokens.input || 0) + (success ? (estimatedTokens.output || 0) : 0)
      };

      await this.costManagement.postRequestProcessing(
        sessionId,
        modelConfig.id,
        actualTokens,
        latency,
        success,
        error
      );
    }

    // ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
    this.updateMetrics(modelConfig.tier, response);

    // Redisãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
    const cost = response.cost_info?.total_cost_usd || 0;
    await this.updateRequestMetrics(modelConfig, latency, cost, success);

    console.log(`[LLMOrchestrator] ${success ? 'âœ…' : 'âŒ'} Request completed: ${modelConfig.id}, Latency: ${latency}ms`);

    return response;
  }

  private checkBudget(modelConfig: ModelConfig): boolean {
    const currentUtilization = this.monthlySpend / this.config.cost_management.monthly_budget_usd;
    
    if (currentUtilization >= this.config.cost_management.cost_alerts.critical_threshold) {
      return false;
    }
    
    if (currentUtilization >= this.config.cost_management.cost_alerts.warning_threshold) {
      console.warn(`[LLMOrchestrator] ğŸ’° Warning: Budget utilization at ${(currentUtilization * 100).toFixed(1)}%`);
    }
    
    return true;
  }

  private shouldCascade(response: LLMResponse, model: ModelConfig): boolean {
    if (!this.config.collaboration.cascade_enabled) return false;
    
    // å“è³ªé–¾å€¤ãƒã‚§ãƒƒã‚¯
    const qualityThresholds = this.config.collaboration.quality_thresholds;
    
    if (!response.success) return true;
    if (!response.response_text || response.response_text.length < qualityThresholds.min_response_length) return true;
    if (response.metadata.confidence_score && response.metadata.confidence_score < qualityThresholds.min_confidence_score) return true;
    
    return false;
  }

  private shouldRefine(response: LLMResponse, model: ModelConfig): boolean {
    if (!this.config.collaboration.refinement_enabled) return false;
    if (model.tier >= 3) return false; // æœ€é«˜Tierã¯æ´—ç·´åŒ–ã—ãªã„
    if (!response.success) return false;
    
    // Tier0ã®å ´åˆã¯ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã§ã®ã¿æ´—ç·´åŒ–ã‚’æ¤œè¨
    if (model.tier === 0) {
      return response.response_text?.includes('```') || false; // ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
    }
    
    return false;
  }

  /**
   * ğŸ†• ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
   * ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘åº¦ãŒä¸ŠãŒã£ãŸå ´åˆã®å‡¦ç†
   */
  private escalateTaskType(currentTaskType: TaskType): TaskType {
    const escalationMap: Record<TaskType, TaskType> = {
      'general': 'complex_analysis',
      'coding': 'premium',
      'complex_analysis': 'premium',
      'premium': 'critical',
      'critical': 'critical', // æ—¢ã«æœ€é«˜ãƒ¬ãƒ™ãƒ«
      'auto': 'complex_analysis',
      // ãã®ä»–ã®ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚‚ãƒãƒƒãƒ”ãƒ³ã‚°
      'rag_search': 'complex_analysis',
      'document_query': 'premium',
      'semantic_search': 'complex_analysis',
      'vector_upsert': 'coding',
      'vector_delete': 'coding',
      'file_search': 'complex_analysis',
      'code_interpreter': 'premium',
      'general_assistant': 'complex_analysis',
      'code_execution': 'premium',
      'assistant_file_search': 'complex_analysis',
      'assistant_code_interpreter': 'premium',
      'assistant_chat': 'complex_analysis'
    };

    return escalationMap[currentTaskType] || currentTaskType;
  }

  /**
   * ğŸš« Gemini Flashä½ç²¾åº¦å¯¾ç­–: å¼·åˆ¶Tierã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡
   * ç”»ä¸€çš„ãªæ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ã§ã¯ãªãã€å¤šæ¬¡å…ƒåˆ†æã«ã‚ˆã‚‹çŸ¥çš„åˆ¤å®š
   */
  private evaluateForcedTierEscalation(
    taskType: TaskType, 
    analysis: QueryAnalysis, 
    request: LLMRequest
  ): {
    minTier: number;
    forcedModel?: string;
    suppressLowTier: boolean;
    reasoning: string;
  } {
    const prompt = request.prompt.toLowerCase();
    let escalationScore = 0;
    const reasons: string[] = [];

    // 1. æ˜ç¤ºçš„ãªé«˜å“è³ªè¦æ±‚ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
    const premiumTasks: Record<TaskType, number> = {
      'premium': 2,
      'critical': 3,
      'complex_analysis': 2,
      'coding': 1, // ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚‚ä¸­ç¨‹åº¦ã®å“è³ªè¦æ±‚
      'general': 0,
      'auto': 0,
      'rag_search': 1,
      'document_query': 1,
      'semantic_search': 1,
      'vector_upsert': 0,
      'vector_delete': 0,
      'file_search': 1,
      'code_interpreter': 2,
      'general_assistant': 1,
      'code_execution': 2,
      'assistant_file_search': 2,
      'assistant_code_interpreter': 2,
      'assistant_chat': 1
    };

    const taskTypeTier = premiumTasks[taskType] || 0;
    if (taskTypeTier > 0) {
      escalationScore += taskTypeTier;
      reasons.push(`TaskType ${taskType} requires Tier ${taskTypeTier}+`);
    }

    // 2. æŠ€è¡“å°‚é–€ç”¨èªã«ã‚ˆã‚‹å“è³ªè¦æ±‚æ¤œå‡ºï¼ˆæ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ã§ã¯ãªãæ„å‘³è«–çš„åˆ†æï¼‰
    const technicalDomains = {
      clustering: ['pacemaker', 'stonith', 'cluster', 'failover', 'ha', 'heartbeat'],
      database: ['postgresql', 'deadlock', 'transaction', 'index', 'query optimization', 'performance tuning'],
      containerization: ['docker', 'kubernetes', 'container', 'systemd', 'privileged'],
      automation: ['jenkins', 'ansible', 'ci/cd', 'pipeline', 'groovy', 'playbook'],
      networking: ['firewall', 'iptables', 'vlan', 'dns', 'routing', 'packet'],
      system_admin: ['apache', 'httpd', 'ssl', 'certificate', 'systemctl', 'journalctl']
    };

    let technicalDomainCount = 0;
    let criticalKeywordCount = 0;

    for (const [domain, keywords] of Object.entries(technicalDomains)) {
      const matchCount = keywords.filter(keyword => prompt.includes(keyword)).length;
      if (matchCount > 0) {
        technicalDomainCount++;
        criticalKeywordCount += matchCount;
      }
    }

    if (technicalDomainCount >= 2) {
      escalationScore += 2;
      reasons.push(`Multi-domain technical complexity detected (${technicalDomainCount} domains)`);
    } else if (technicalDomainCount >= 1) {
      escalationScore += 1;
      reasons.push(`Technical domain specialization required`);
    }

    // 3. è¤‡é›‘åº¦ãƒ»å“è³ªåˆ†æã«åŸºã¥ãåˆ¤å®š
    if (analysis.complexity === 'expert' || analysis.complexity === 'complex') {
      escalationScore += 2;
      reasons.push(`High complexity analysis: ${analysis.complexity}`);
    }

    if (analysis.qualityRequirement === 'exceptional' || analysis.qualityRequirement === 'high') {
      escalationScore += 1;
      reasons.push(`Quality requirement: ${analysis.qualityRequirement}`);
    }

    if (analysis.reasoningDepth === 'deep') {
      escalationScore += 1;
      reasons.push(`Deep reasoning required`);
    }

    // 4. ã‚¨ãƒ©ãƒ¼è§£æãƒ»è¨ºæ–­è¦æ±‚æ¤œå‡º
    const diagnosticKeywords = [
      'error', 'fail', 'troubleshoot', 'debug', 'analyze', 'investigate',
      'ã‚¨ãƒ©ãƒ¼', 'å¤±æ•—', 'å•é¡Œ', 'è§£æ', 'èª¿æŸ»', 'è¨ºæ–­'
    ];
    const hasDiagnostic = diagnosticKeywords.some(keyword => prompt.includes(keyword));
    if (hasDiagnostic) {
      escalationScore += 1;
      reasons.push('Diagnostic/troubleshooting request requires precision');
    }

    // 5. æœ€çµ‚åˆ¤å®š
    let minTier = 1; // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    let forcedModel: string | undefined;
    let suppressLowTier = false;

    if (escalationScore >= 5) {
      minTier = 3;
      forcedModel = 'gpt4o'; // æœ€é«˜å“è³ªç¢ºç´„
      suppressLowTier = true;
      reasons.push('CRITICAL: Forced GPT-4o selection');
    } else if (escalationScore >= 3) {
      minTier = 2;
      suppressLowTier = true;
      reasons.push('HIGH: Tier 2+ required, Flash suppressed');
    } else if (escalationScore >= 2) {
      minTier = 2;
      reasons.push('MEDIUM: Tier 2+ preferred');
    } else if (taskType === 'premium' || taskType === 'critical') {
      // æ˜ç¤ºçš„æŒ‡å®šã¯å¿…ãšå®ˆã‚‹
      minTier = premiumTasks[taskType];
      suppressLowTier = true;
      reasons.push(`Explicit ${taskType} task type honored`);
    }

    const reasoning = reasons.join('; ');
    console.log(`[LLMOrchestrator] ğŸ”¬ Escalation Analysis: Score=${escalationScore}, MinTier=${minTier}, Reasoning=[${reasoning}]`);

    return {
      minTier,
      forcedModel,
      suppressLowTier,
      reasoning
    };
  }

  private async cascadeToHigherTier(
    request: LLMRequest, 
    failedModel: ModelConfig, 
    failedResponse: LLMResponse
  ): Promise<LLMResponse> {
    const higherTierModels = Object.values(this.config.models)
      .filter(model => model.tier > failedModel.tier)
      .filter(model => this.clients.has(model.id))
      .sort((a, b) => a.tier - b.tier); // ä½ã„Tierã‹ã‚‰é †ã«

    if (higherTierModels.length === 0) {
      console.log('[LLMOrchestrator] No higher tier models available for cascade');
      return failedResponse; // å…ƒã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
    }

    const nextModel = higherTierModels[0];
    console.log(`[LLMOrchestrator] Cascading from Tier ${failedModel.tier} to Tier ${nextModel.tier}`);
    
    const cascadeResponse = await this.executeRequest(request, nextModel);
    cascadeResponse.performance_info.fallback_used = true;
    cascadeResponse.performance_info.tier_escalation = true;
    
    return cascadeResponse;
  }

  private async refineWithHigherTier(request: LLMRequest, baseResponse: LLMResponse): Promise<LLMResponse> {
    const higherTierModels = Object.values(this.config.models)
      .filter(model => model.tier > baseResponse.tier_used)
      .filter(model => this.clients.has(model.id))
      .sort((a, b) => a.tier - b.tier);

    if (higherTierModels.length === 0) {
      return baseResponse;
    }

    const refinementModel = higherTierModels[0];
    const refinementPrompt = `
Please review and improve the following response:

Original Prompt: ${request.prompt}

Response to Improve:
${baseResponse.response_text}

Please provide an improved version that:
1. Maintains accuracy and correctness
2. Improves clarity and structure
3. Adds helpful details where appropriate
4. Fixes any issues or errors

Improved Response:`;

    console.log(`[LLMOrchestrator] Refining with Tier ${refinementModel.tier} model`);
    const refinedResponse = await this.executeRequest({ prompt: refinementPrompt }, refinementModel);
    
    // å…ƒã®ã‚³ã‚¹ãƒˆæƒ…å ±ã¨åˆè¨ˆã™ã‚‹
    refinedResponse.cost_info.total_cost_usd += baseResponse.cost_info.total_cost_usd;
    refinedResponse.performance_info.tier_escalation = true;
    
    return refinedResponse;
  }

  private updateMetrics(tier: number, response: LLMResponse): void {
    this.metrics.requests_per_tier[tier]++;
    
    if (response.success) {
      this.metrics.success_rate_per_tier[tier] = 
        (this.metrics.success_rate_per_tier[tier] + 1) / this.metrics.requests_per_tier[tier];
    }
    
    this.metrics.average_latency_per_tier[tier] = 
      (this.metrics.average_latency_per_tier[tier] + response.performance_info.latency_ms) / this.metrics.requests_per_tier[tier];
    
    this.metrics.cost_per_tier[tier] += response.cost_info.total_cost_usd;
    this.monthlySpend += response.cost_info.total_cost_usd;
    
    this.metrics.total_monthly_spend = this.monthlySpend;
    this.metrics.budget_utilization_percentage = 
      (this.monthlySpend / this.config.cost_management.monthly_budget_usd) * 100;
  }

  // å…¬é–‹ãƒ¡ã‚½ãƒƒãƒ‰
  public async healthCheck(): Promise<{ healthy: boolean; details: Record<string, boolean> }> {
    console.log('[LLMOrchestrator] Performing health check on all clients...');
    
    const healthResults: Record<string, boolean> = {};
    
    for (const [modelId, client] of this.clients.entries()) {
      try {
        healthResults[modelId] = await client.isHealthy();
      } catch (error) {
        console.error(`[LLMOrchestrator] Health check failed for ${modelId}:`, error);
        healthResults[modelId] = false;
      }
    }
    
    const healthyCount = Object.values(healthResults).filter(Boolean).length;
    const totalCount = Object.keys(healthResults).length;
    
    console.log(`[LLMOrchestrator] Health check complete: ${healthyCount}/${totalCount} clients healthy`);
    
    return {
      healthy: healthyCount > 0, // å°‘ãªãã¨ã‚‚1ã¤ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒå¥å…¨ã§ã‚ã‚Œã° OK
      details: healthResults
    };
  }

  public getMetrics(): SystemMetrics {
    return { ...this.metrics };
  }

  // Vector Storageç­‰ã®æ–°æ©Ÿèƒ½ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åˆ¤å®š
  private isCapabilityRequest(request: LLMRequest): boolean {
    const capabilityTaskTypes = [
      'rag_search', 'document_query', 'semantic_search', 
      'vector_upsert', 'vector_delete'
      // å°†æ¥çš„ã«file_search, code_executionã‚‚è¿½åŠ 
    ];
    
    return capabilityTaskTypes.includes(request.task_type || '');
  }

  // CapabilityProviderã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†
  private async processWithCapabilityProvider(request: LLMRequest): Promise<LLMResponse> {
    console.log(`[LLMOrchestrator] ğŸ”§ Processing with capability provider: ${request.task_type}`);
    
    const startTime = Date.now();
    
    try {
      // æœ€é©ãªCapabilityProviderã‚’é¸æŠ
      const { provider, routing } = this.capabilityRegistry.findBestProviderWithRouting(request);
      
      if (!provider) {
        throw new Error(`No suitable capability provider found for task type: ${request.task_type}`);
      }

      console.log(`[LLMOrchestrator] Selected capability provider: ${provider.name}`);
      console.log(`[LLMOrchestrator] Routing info: ${routing.selection_reason}`);

      // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ
      const response = await provider.execute(request);
      
      const latency = Date.now() - startTime;

      // ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
      this.capabilityRegistry.updateMetrics(
        provider.name,
        response.success,
        latency,
        response.cost_info.total_cost_usd
      );

      // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ä»˜åŠ æƒ…å ±ã‚’è¿½åŠ 
      response.metadata = {
        ...response.metadata,
        routing_info: routing,
        capability_provider: provider.name
      };

      console.log(`[LLMOrchestrator] âœ… Capability request completed: ${provider.name}, Success: ${response.success}, Cost: $${response.cost_info.total_cost_usd.toFixed(6)}`);
      
      return response;

    } catch (error) {
      console.error('[LLMOrchestrator] âŒ Capability request failed:', error);
      
      const errorResponse: LLMResponse = {
        success: false,
        model_used: 'capability_error',
        tier_used: -1,
        error: {
          code: 'CAPABILITY_ERROR',
          message: error instanceof Error ? error.message : 'Unknown capability error'
        },
        metadata: {
          model_id: 'capability_error',
          provider: 'system',
          tokens_used: { input: 0, output: 0, total: 0 },
          generated_at: new Date().toISOString(),
          tier_used: -1,
          processing_time_ms: 0,
          estimated_complexity: 0
        },
        cost_info: {
          total_cost_usd: 0,
          input_cost_usd: 0,
          output_cost_usd: 0
        },
        performance_info: {
          latency_ms: Date.now() - startTime,
          processing_time_ms: Date.now() - startTime,
          fallback_used: false
        }
      };

      return errorResponse;
    }
  }

  public getAvailableModels(): ModelConfig[] {
    return Object.values(this.config.models).filter(model => 
      this.clients.has(model.id)
    );
  }

  public resetMetrics(): void {
    this.initializeMetrics();
    this.monthlySpend = 0;
    this.requestCount = 0;
    console.log('[LLMOrchestrator] Metrics reset');
  }

  // å”èª¿ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰
  public async processCollaborativeCoding(request: DecompositionRequest): Promise<CodingSession> {
    const sessionId = `session_${Date.now()}_${Math.random().toString(36).substring(7)}`;
    console.log(`\n[LLMOrchestrator] ğŸ¤ Starting collaborative coding session: ${sessionId}`);
    console.log(`[LLMOrchestrator] Original prompt: "${request.originalPrompt.substring(0, 150)}${request.originalPrompt.length > 150 ? '...' : ''}"`);
    
    const session: CodingSession = {
      sessionId,
      originalRequest: request.originalPrompt,
      decomposition: {} as DecompositionResult, // ä¸€æ™‚çš„ã«ç©ºã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
      subtasks: [],
      progress: {
        completed: 0,
        inProgress: 0,
        failed: 0,
        total: 0
      },
      metrics: {
        totalProcessingTime: 0,
        qwen3Usage: 0,
        claudeUsage: 0,
        totalCost: 0,
        qualityScore: 0
      },
      status: 'planning',
      startTime: new Date().toISOString()
    };

    this.activeSessions.set(sessionId, session);

    // ã‚³ã‚¹ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ã‚’ç™»éŒ²
    if (this.costManagement) {
      await this.costManagement.tracker.startSession(sessionId, {
        user_id: request.context || 'system',
        project_id: 'collaborative-coding',
        prompt: request.originalPrompt.substring(0, 500)
      });
    }

    try {
      // Step 1: ã‚¿ã‚¹ã‚¯åˆ†è§£
      console.log(`[LLMOrchestrator] Step 1: Task decomposition`);
      const decomposition = await this.taskDecomposer.decompose(request);
      session.decomposition = decomposition;
      session.subtasks = [...decomposition.subtasks];
      session.progress.total = decomposition.subtasks.length;

      // Step 2: é›£æ˜“åº¦å†è©•ä¾¡
      console.log(`[LLMOrchestrator] Step 2: Difficulty classification`);
      const classifiedSubtasks = await this.difficultyClassifier.classifyBatch(session.subtasks);
      session.subtasks = classifiedSubtasks;

      // Step 3: ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®ä¸¦åˆ—å®Ÿè¡Œ
      console.log(`[LLMOrchestrator] Step 3: Executing subtasks`);
      session.status = 'executing';
      await this.executeSubtasks(session);

      // Step 4: å…¨ä½“çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯ã¨çµ±åˆ
      console.log(`[LLMOrchestrator] Step 4: Final quality check and integration`);
      session.status = 'reviewing';
      await this.performFinalQualityCheck(session);

      session.status = 'completed';
      session.endTime = new Date().toISOString();
      
      // ã‚³ã‚¹ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†ã‚’è¨˜éŒ²
      if (this.costManagement) {
        try {
          const finalSession = await this.costManagement.tracker.endSession(sessionId);
          console.log(`[LLMOrchestrator] ğŸ’° Final session cost: $${finalSession.total_cost.total_cost_usd.toFixed(6)}`);
          console.log(`[LLMOrchestrator] ğŸ“Š Total requests: ${finalSession.total_requests}, Tokens: ${finalSession.total_tokens.total.toLocaleString()}`);
        } catch (error) {
          console.warn(`[LLMOrchestrator] âš ï¸ Failed to finalize session cost tracking:`, error);
        }
      }
      
      console.log(`[LLMOrchestrator] ğŸ‰ Collaborative coding session completed: ${sessionId}`);
      console.log(`[LLMOrchestrator] Final metrics: Cost=$${session.metrics.totalCost.toFixed(4)}, Quality=${session.metrics.qualityScore.toFixed(1)}`);
      
      return session;

    } catch (error) {
      console.error(`[LLMOrchestrator] âŒ Collaborative coding session failed:`, error);
      session.status = 'failed';
      session.endTime = new Date().toISOString();
      throw error;
    }
  }

  private async executeSubtasks(session: CodingSession): Promise<void> {
    const easyTasks = session.subtasks.filter(t => t.difficulty === 'easy');
    const hardTasks = session.subtasks.filter(t => t.difficulty === 'hard');

    console.log(`[LLMOrchestrator] Executing ${easyTasks.length} easy tasks and ${hardTasks.length} hard tasks`);

    // ä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ã—ã¦å®Ÿè¡Œé †åºã‚’æ±ºå®š
    const executionOrder = this.calculateExecutionOrder(session.subtasks);

    for (const subtask of executionOrder) {
      console.log(`[LLMOrchestrator] Processing subtask: ${subtask.id} (${subtask.difficulty})`);
      subtask.status = 'in_progress';
      session.progress.inProgress++;

      try {
        const result = await this.executeSubtask(subtask, session);
        subtask.result = result;
        
        // å“è³ªãƒã‚§ãƒƒã‚¯
        const qualityReview = await this.qualityGate.review(subtask, result);
        
        if (qualityReview.requiresRevision && subtask.retryCount < this.collaborativeConfig.maxRetries) {
          console.log(`[LLMOrchestrator] Quality check failed, retrying subtask: ${subtask.id}`);
          subtask.status = 'retry';
          subtask.retryCount++;
          subtask.feedback = qualityReview.comments;
          
          // å†å®Ÿè¡Œï¼ˆå¿…è¦ã«å¿œã˜ã¦Claude Codeã¸ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
          const retryResult = await this.retrySubtask(subtask, session, qualityReview);
          subtask.result = retryResult;
        }

        if (qualityReview.passed || subtask.retryCount >= this.collaborativeConfig.maxRetries) {
          subtask.status = 'done';
          session.progress.completed++;
          session.progress.inProgress--;
        } else {
          subtask.status = 'failed';
          session.progress.failed++;
          session.progress.inProgress--;
        }

        // ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
        session.metrics.totalCost += result.metadata.tokens_used * 0.001; // æ¦‚ç®—ã‚³ã‚¹ãƒˆ
        if (result.metadata.model_used.includes('qwen')) {
          session.metrics.qwen3Usage++;
        } else {
          session.metrics.claudeUsage++;
        }

      } catch (error) {
        console.error(`[LLMOrchestrator] Subtask execution failed: ${subtask.id}`, error);
        subtask.status = 'failed';
        session.progress.failed++;
        session.progress.inProgress--;
      }
    }
  }

  private async executeSubtask(subtask: Subtask, session: CodingSession): Promise<CodeResult> {
    if (subtask.difficulty === 'easy') {
      // Qwen3 Coderã§å®Ÿè¡Œï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Ÿéš›ã®IDã‚’ç¢ºèªï¼‰
      const qwenModels = Object.entries(this.config.models).filter(([id, config]) => config.tier === 0);
      if (qwenModels.length > 0) {
        const [qwenId, qwenConfig] = qwenModels[0];
        const qwenClient = this.clients.get(qwenId);
        if (qwenClient) {
          console.log(`[LLMOrchestrator] ğŸš€ Delegating to Qwen3 Coder: ${subtask.id}`);
          const response = await qwenClient.generate(subtask.description);
          
          return {
            code: response.response_text || '',
            explanation: `Generated by Qwen3 Coder for: ${subtask.description}`,
            metadata: {
              model_used: qwenId,
              tier_used: 0,
              tokens_used: response.metadata.tokens_used?.total || 0,
              processing_time_ms: response.performance_info.processing_time_ms,
              confidence_score: response.metadata.confidence_score,
              estimated_complexity: 3 // Easy task baseline
            }
          };
        }
      }
    }
    
    // Claude Codeã§å®Ÿè¡Œï¼ˆhardã‚¿ã‚¹ã‚¯ã¾ãŸã¯Qwen3ãŒåˆ©ç”¨ä¸å¯ã®å ´åˆï¼‰
    console.log(`[LLMOrchestrator] ğŸ§  Executing with Claude Code: ${subtask.id}`);
    
    // åˆ©ç”¨å¯èƒ½ãªæœ€é«˜Tierã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’æ¢ã™
    const availableClients = Array.from(this.clients.keys());
    console.log(`[LLMOrchestrator] Available clients: ${availableClients.join(', ')}`);
    
    if (availableClients.length > 0) {
      const clientId = availableClients[0]; // æœ€åˆã®åˆ©ç”¨å¯èƒ½ãªã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
      const client = this.clients.get(clientId)!;
      const response = await client.generate(subtask.description);
      
      return {
        code: response.response_text || '',
        explanation: `Generated by ${clientId} for: ${subtask.description}`,
        metadata: {
          model_used: clientId,
          tier_used: this.config.models[clientId]?.tier || 0,
          tokens_used: response.metadata.tokens_used?.total || 0,
          processing_time_ms: response.performance_info.processing_time_ms,
          confidence_score: response.metadata.confidence_score,
          estimated_complexity: subtask.difficulty === 'hard' ? 7 : 4
        }
      };
    }

    throw new Error(`No suitable client available for subtask: ${subtask.id}`);
  }

  private async retrySubtask(subtask: Subtask, session: CodingSession, qualityReview: any): Promise<CodeResult> {
    const shouldEscalate = this.collaborativeConfig.autoEscalateToClaudeAfterRetries && 
                          subtask.difficulty === 'easy' && 
                          subtask.retryCount >= 1;

    if (shouldEscalate) {
      console.log(`[LLMOrchestrator] ğŸ“ˆ Escalating to Claude Code: ${subtask.id}`);
      subtask.difficulty = 'hard'; // é›£æ˜“åº¦ã‚’ä¸€æ™‚çš„ã«å¤‰æ›´
      return this.executeSubtask(subtask, session);
    } else {
      console.log(`[LLMOrchestrator] ğŸ”„ Retrying with same model: ${subtask.id}`);
      const improvedPrompt = `${subtask.description}

Previous attempt had these issues:
${qualityReview.comments}

Please address these issues and provide an improved implementation.`;
      
      subtask.description = improvedPrompt;
      return this.executeSubtask(subtask, session);
    }
  }

  private calculateExecutionOrder(subtasks: Subtask[]): Subtask[] {
    // ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆã§ä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ã—ãŸå®Ÿè¡Œé †åºã‚’æ±ºå®š
    const visited = new Set<string>();
    const result: Subtask[] = [];
    
    const visit = (task: Subtask) => {
      if (visited.has(task.id)) return;
      
      visited.add(task.id);
      
      // ä¾å­˜é–¢ä¿‚ã®ã‚ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å…ˆã«å®Ÿè¡Œ
      if (task.dependencies) {
        for (const depId of task.dependencies) {
          const depTask = subtasks.find(t => t.id === depId);
          if (depTask) {
            visit(depTask);
          }
        }
      }
      
      result.push(task);
    };
    
    for (const task of subtasks) {
      visit(task);
    }
    
    return result;
  }

  private async performFinalQualityCheck(session: CodingSession): Promise<void> {
    console.log(`[LLMOrchestrator] Performing final quality check for session: ${session.sessionId}`);
    
    const completedTasks = session.subtasks.filter(t => t.status === 'done' && t.result);
    let totalQualityScore = 0;
    
    for (const task of completedTasks) {
      if (task.result) {
        const review = await this.qualityGate.review(task, task.result);
        totalQualityScore += review.score;
      }
    }
    
    session.metrics.qualityScore = completedTasks.length > 0 ? totalQualityScore / completedTasks.length : 0;
  }

  public getActiveSession(sessionId: string): CodingSession | undefined {
    return this.activeSessions.get(sessionId);
  }

  public listActiveSessions(): CodingSession[] {
    return Array.from(this.activeSessions.values());
  }

  // ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
  private shouldUseCollaborativeCoding(request: LLMRequest): boolean {
    // ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ãŒæ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’å°Šé‡
    if (request.task_type) {
      // åˆ†æç³»ã‚¿ã‚¹ã‚¯ã¯å”èª¿ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ã‚ãªã„
      const analysisTaskTypes = ['complex_analysis', 'general', 'premium', 'critical'];
      if (analysisTaskTypes.includes(request.task_type)) {
        return false;
      }
      // ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç³»ã‚¿ã‚¹ã‚¯ã®ã¿å”èª¿ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
      if (request.task_type === 'coding') {
        return true;
      }
    }

    const prompt = request.prompt.toLowerCase();
    
    // åˆ†æç³»ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å”èª¿ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ã‚ãªã„
    const analysisKeywords = [
      'analysis', 'analyze', 'explain', 'describe', 'discuss', 'theory', 'theorem',
      'mathematical', 'proof', 'demonstrate', 'show', 'calculate', 'derive',
      'åˆ†æ', 'è§£æ', 'èª¬æ˜', 'è§£èª¬', 'è€ƒå¯Ÿ', 'ç†è«–', 'å®šç†', 'è¨¼æ˜', 
      'æ•°å­¦', 'è¨ˆç®—', 'å°å‡º', 'æ¤œè¨', 'ç ”ç©¶'
    ];
    
    const hasAnalysisKeywords = analysisKeywords.some(keyword => prompt.includes(keyword));
    if (hasAnalysisKeywords) {
      return false;
    }
    
    // ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã®æ¤œå‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    const codingKeywords = [
      'implement', 'create', 'build', 'develop', 'code', 'function', 'class', 
      'api', 'endpoint', 'component', 'module', 'service', 'algorithm',
      'å®Ÿè£…', 'ä½œæˆ', 'é–‹ç™º', 'æ§‹ç¯‰', 'ã‚³ãƒ¼ãƒ‰', 'é–¢æ•°', 'ã‚¯ãƒ©ã‚¹', 'API',
      'ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ', 'ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«', 'ã‚µãƒ¼ãƒ“ã‚¹', 'ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ '
    ];
    
    const hasCodeKeywords = codingKeywords.some(keyword => prompt.includes(keyword));
    const hasCodeBlocks = prompt.includes('```') || prompt.includes('function') || prompt.includes('class ');
    
    // é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚‚ã€åˆ†æç³»ã§ãªãã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´ ãŒã‚ã‚‹å ´åˆã®ã¿å”èª¿ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
    const isLongCodingTask = request.prompt.length > 400 && hasCodeKeywords;
    
    return hasCodeKeywords || hasCodeBlocks || isLongCodingTask;
  }

  private extractTargetLanguage(prompt: string): string {
    const prompt_lower = prompt.toLowerCase();
    
    const languageMap: Record<string, string> = {
      'typescript': 'typescript',
      'javascript': 'javascript', 
      'python': 'python',
      'java': 'java',
      'go': 'go',
      'rust': 'rust',
      'cpp': 'cpp',
      'c++': 'cpp'
    };
    
    for (const [keyword, language] of Object.entries(languageMap)) {
      if (prompt_lower.includes(keyword)) {
        return language;
      }
    }
    
    return 'typescript'; // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
  }

  private convertSessionToResponse(session: CodingSession, startTime: number): LLMResponse {
    const completedTasks = session.subtasks.filter(t => t.status === 'done');
    const combinedCode = completedTasks.map(t => t.result?.code || '').join('\n\n');
    const combinedExplanation = completedTasks.map(t => 
      `${t.id}: ${t.result?.explanation || t.description}`
    ).join('\n');

    return {
      success: session.status === 'completed',
      model_used: 'collaborative_pipeline',
      tier_used: -1, // è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
      response_text: `${combinedExplanation}\n\n--- Generated Code ---\n${combinedCode}`,
      metadata: {
        model_id: 'collaborative_pipeline',
        provider: 'hybrid',
        tokens_used: {
          input: session.subtasks.reduce((sum, t) => sum + (t.result?.metadata.tokens_used || 0), 0),
          output: session.subtasks.reduce((sum, t) => sum + (t.result?.metadata.tokens_used || 0), 0),
          total: session.subtasks.reduce((sum, t) => sum + (t.result?.metadata.tokens_used || 0), 0)
        },
        generated_at: new Date().toISOString(),
        confidence_score: session.metrics.qualityScore / 100,
        session_id: session.sessionId,
        subtasks_completed: completedTasks.length,
        qwen3_usage: session.metrics.qwen3Usage,
        claude_usage: session.metrics.claudeUsage,
        tier_used: -1,
        processing_time_ms: Date.now() - startTime,
        estimated_complexity: session.subtasks.length
      },
      cost_info: {
        total_cost_usd: session.metrics.totalCost,
        input_cost_usd: session.metrics.totalCost * 0.6,
        output_cost_usd: session.metrics.totalCost * 0.4
      },
      performance_info: {
        latency_ms: Date.now() - startTime,
        processing_time_ms: session.metrics.totalProcessingTime,
        fallback_used: false,
        collaborative_session: true
      }
    };
  }

  // ============================================
  // Redisçµ±åˆãƒ­ã‚°ãƒ¡ã‚½ãƒƒãƒ‰
  // ============================================
  
  /**
   * ã‚¯ã‚¨ãƒªåˆ†æçµæœã‚’Redisã«ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ­ã‚°ã¨ã—ã¦è¨˜éŒ²
   */
  private async logQueryAnalysis(
    requestId: string, 
    request: LLMRequest, 
    analysis: QueryAnalysis, 
    selectedModel: ModelConfig, 
    taskType: TaskType
  ): Promise<void> {
    if (!this.redisLogger) return;

    try {
      const analysisTime = Date.now();
      
      // ä»£æ›¿ãƒ¢ãƒ‡ãƒ«å€™è£œã‚’å–å¾—
      const alternativeModels = Object.values(this.config.models)
        .filter(model => model.id !== selectedModel.id)
        .sort((a, b) => this.calculateModelScore(analysis, b) - this.calculateModelScore(analysis, a))
        .slice(0, 3)
        .map(model => model.id);

      const queryAnalysisLog: QueryAnalysisLog = {
        requestId,
        timestamp: analysisTime,
        complexity: analysis.complexity,
        reasoning_depth: analysis.reasoningDepth,
        creativity_level: analysis.creativityLevel,
        routing_decision: taskType,
        selected_model: selectedModel.id,
        selected_tier: selectedModel.tier,
        confidence_score: this.calculateConfidenceScore(analysis, selectedModel),
        alternative_models: alternativeModels,
        analysis_time_ms: 0, // å¾Œã§è¨ˆç®—
        prompt_length: request.prompt.length,
        estimated_cost: this.estimateRequestCost(selectedModel, request.prompt.length),
        priority_balance: analysis.priorityBalance
      };

      await this.redisLogger.logQueryAnalysis(queryAnalysisLog);
      
      console.log(`[LLMOrchestrator] ğŸ“Š Query analysis logged to Redis: ${requestId}`);
    } catch (error) {
      console.warn('[LLMOrchestrator] Failed to log query analysis:', error);
    }
  }

  /**
   * åˆ†æçµæœã«åŸºã¥ããƒ¢ãƒ‡ãƒ«ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå¤šæ¬¡å…ƒè©•ä¾¡ï¼‰
   */
  private calculateModelScore(analysis: QueryAnalysis, model: ModelConfig): number {
    let score = 0;
    
    // 1. è¤‡é›‘åº¦é©åˆåº¦è©•ä¾¡ (0-3ç‚¹)
    const complexityMapping = {
      'trivial': { tier0: 3, tier1: 2, tier2: 1, tier3: 0 },
      'simple': { tier0: 2, tier1: 3, tier2: 2, tier3: 1 },  
      'moderate': { tier0: 1, tier1: 2, tier2: 3, tier3: 2 },
      'complex': { tier0: 0, tier1: 1, tier2: 2, tier3: 3 },
      'expert': { tier0: 0, tier1: 0, tier2: 1, tier3: 3 }
    } as const;
    
    const tierKey = `tier${model.tier}` as keyof typeof complexityMapping['trivial'];
    const complexityScore = complexityMapping[analysis.complexity]?.[tierKey] || 0;
    score += complexityScore;
    
    // 2. æ¨è«–æ·±åº¦é©åˆåº¦è©•ä¾¡ (0-2ç‚¹)
    const reasoningScores = {
      'shallow': { tier0: 2, tier1: 1, tier2: 0, tier3: 0 },
      'moderate': { tier0: 1, tier1: 2, tier2: 2, tier3: 1 },
      'deep': { tier0: 0, tier1: 1, tier2: 2, tier3: 2 }
    } as const;
    
    const reasoningScore = reasoningScores[analysis.reasoningDepth]?.[tierKey] || 0;
    score += reasoningScore;
    
    // 3. å‰µé€ æ€§ãƒ¬ãƒ™ãƒ«é©åˆåº¦è©•ä¾¡ (0-2ç‚¹)
    const creativityScores = {
      'factual': { tier0: 2, tier1: 2, tier2: 1, tier3: 1 },
      'analytical': { tier0: 1, tier1: 2, tier2: 2, tier3: 2 },
      'creative': { tier0: 0, tier1: 1, tier2: 2, tier3: 2 },
      'innovative': { tier0: 0, tier1: 0, tier2: 1, tier3: 2 }
    } as const;
    
    const creativityScore = creativityScores[analysis.creativityLevel]?.[tierKey] || 0;
    score += creativityScore;
    
    // 4. ã‚³ã‚¹ãƒˆåŠ¹ç‡æ€§è©•ä¾¡ (0-2ç‚¹ã€é‡ã¿ä»˜ã‘)
    const costEfficiency = [2, 1.5, 1, 0.5][model.tier] || 0; // tier0ãŒæœ€é«˜åŠ¹ç‡
    const costScore = costEfficiency * analysis.priorityBalance.cost;
    score += costScore;
    
    // 5. é€Ÿåº¦åŠ¹ç‡æ€§è©•ä¾¡ (0-2ç‚¹ã€é‡ã¿ä»˜ã‘)  
    const speedEfficiency = [2, 1.5, 1, 0.5][model.tier] || 0; // tier0ãŒæœ€é«˜é€Ÿåº¦
    const speedScore = speedEfficiency * analysis.priorityBalance.speed;
    score += speedScore;
    
    // 6. ç²¾åº¦é‡è¦åº¦è©•ä¾¡ (0-2ç‚¹ã€é‡ã¿ä»˜ã‘)
    const accuracyBonus = [0.5, 1, 1.5, 2][model.tier] || 0; // tier3ãŒæœ€é«˜ç²¾åº¦
    const accuracyScore = accuracyBonus * analysis.priorityBalance.accuracy;
    score += accuracyScore;
    
    // 7. ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒœãƒ¼ãƒŠã‚¹ (0-1ç‚¹)
    const domainBonus = this.calculateDomainBonus(analysis.domain, model);
    score += domainBonus;
    
    return Math.max(0, score); // è² æ•°å›é¿
  }

  /**
   * ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—
   */
  private calculateDomainBonus(domains: string[], model: ModelConfig): number {
    let bonus = 0;
    
    // ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®ãƒ¢ãƒ‡ãƒ«é©æ€§è©•ä¾¡
    for (const domain of domains) {
      switch (domain.toLowerCase()) {
        case 'coding':
        case 'programming':
        case 'software':
          // Qwen3 Coderã¯ç‰¹ã«ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«å„ªç§€
          if (model.id.includes('qwen') && model.id.includes('coder')) bonus += 0.5;
          if (model.tier === 0) bonus += 0.3; // tier0ã¯ä¸€èˆ¬çš„ã«ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«é©ã—ã¦ã„ã‚‹
          break;
          
        case 'strategy':
        case 'business':  
        case 'analysis':
          // GPT-4o, Claude Sonnetã¯æˆ¦ç•¥åˆ†æã«å„ªç§€
          if (model.id.includes('gpt-4o') || model.id.includes('claude')) bonus += 0.5;
          if (model.tier >= 2) bonus += 0.3; // é«˜tierã¯åˆ†æã«é©ã—ã¦ã„ã‚‹
          break;
          
        case 'creative':
        case 'writing':
        case 'content':
          // Claude, GPTã¯å‰µé€ çš„ã‚¿ã‚¹ã‚¯ã«å„ªç§€
          if (model.id.includes('claude') || model.id.includes('gpt')) bonus += 0.4;
          break;
          
        case 'math':
        case 'calculation':
        case 'logic':
          // æ•°å­¦ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆ
          if (model.id.includes('math') || model.id.includes('reasoning')) bonus += 0.5;
          break;
          
        default:
          // æ±ç”¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã¯ä¸­é–“tierãŒé©ã—ã¦ã„ã‚‹
          if (model.tier === 1 || model.tier === 2) bonus += 0.1;
      }
    }
    
    return Math.min(bonus, 1.0); // æœ€å¤§1.0ç‚¹
  }

  /**
   * ãƒ¢ãƒ‡ãƒ«é¸æŠã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
   */
  private calculateConfidenceScore(analysis: QueryAnalysis, selectedModel: ModelConfig): number {
    const modelScore = this.calculateModelScore(analysis, selectedModel);
    const maxPossibleScore = 12; // æœ€å¤§ã‚¹ã‚³ã‚¢ï¼ˆ3+2+2+2+2+1 = 12ç‚¹ï¼‰
    return Math.min(modelScore / maxPossibleScore, 1.0);
  }

  /**
   * ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚³ã‚¹ãƒˆæ¨å®š
   */
  private estimateRequestCost(model: ModelConfig, promptLength: number): number {
    const estimatedTokens = Math.ceil(promptLength / 4) + 100; // å…¥åŠ›+å‡ºåŠ›ã®æ¦‚ç®—
    const inputCost = (estimatedTokens * 0.7) * (model.cost_per_1k_tokens.input / 1000);
    const outputCost = (estimatedTokens * 0.3) * (model.cost_per_1k_tokens.output / 1000);
    return inputCost + outputCost;
  }

  /**
   * ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Œäº†å¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
   */
  private async updateRequestMetrics(
    selectedModel: ModelConfig,
    latency: number,
    cost: number,
    success: boolean,
    requestId?: string
  ): Promise<void> {
    if (!this.redisLogger) return;

    try {
      // ãƒ¢ãƒ‡ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
      await this.redisLogger.updateModelMetrics(selectedModel.id, latency, cost, success);
      
      // ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
      if (!success) {
        await this.redisLogger.trackError(
          'request_execution_failed',
          `Request failed for model ${selectedModel.id}`,
          { requestId, model: selectedModel.id, latency, cost }
        );
      }
      
      console.log(`[LLMOrchestrator] ğŸ“ˆ Metrics updated for ${selectedModel.id}`);
    } catch (error) {
      console.warn('[LLMOrchestrator] Failed to update request metrics:', error);
    }
  }

  /**
   * æ—¥æ¬¡ã‚³ã‚¹ãƒˆæ›´æ–°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é–‹å§‹
   */
  private startDailyCostScheduler(): void {
    console.log('[LLMOrchestrator] Starting daily cost table scheduler...');
    
    // æ¯æ—¥åˆå‰0:05ã«ã‚³ã‚¹ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«æ›´æ–°
    const scheduleNextUpdate = () => {
      const now = new Date();
      const nextUpdate = new Date(now);
      nextUpdate.setHours(0, 5, 0, 0); // åˆå‰0:05
      
      // ä»Šæ—¥ã®0:05ã‚’éãã¦ã„ãŸã‚‰ç¿Œæ—¥ã«è¨­å®š
      if (nextUpdate <= now) {
        nextUpdate.setDate(nextUpdate.getDate() + 1);
      }
      
      const timeUntilUpdate = nextUpdate.getTime() - now.getTime();
      
      console.log(`[LLMOrchestrator] ğŸ“… Next daily cost update scheduled: ${nextUpdate.toISOString()}`);
      
      setTimeout(async () => {
        console.log('[LLMOrchestrator] ğŸ•’ Daily cost table update triggered');
        
        if (this.redisLogger) {
          try {
            await this.redisLogger.updateDailyCosts();
            console.log('[LLMOrchestrator] âœ… Daily cost table updated successfully');
          } catch (error) {
            console.error('[LLMOrchestrator] âŒ Daily cost table update failed:', error);
          }
        }
        
        // æ¬¡ã®æ›´æ–°ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
        scheduleNextUpdate();
      }, timeUntilUpdate);
    };
    
    scheduleNextUpdate();
    
    // èµ·å‹•æ™‚ã«ã‚‚å³åº§ã«æ›´æ–°
    setTimeout(async () => {
      if (this.redisLogger) {
        try {
          await this.redisLogger.updateDailyCosts();
          console.log('[LLMOrchestrator] âœ… Initial daily cost table updated');
        } catch (error) {
          console.warn('[LLMOrchestrator] âš ï¸ Initial daily cost table update failed:', error);
        }
      }
    }, 5000); // 5ç§’å¾Œã«å®Ÿè¡Œï¼ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¾Œï¼‰
  }

  /**
   * ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ï¼ˆå¤–éƒ¨APIç”¨ï¼‰
   */
  async getRealTimeMetrics(): Promise<any> {
    if (!this.redisLogger) {
      return { error: 'Redis Logger not available' };
    }
    
    return await this.redisLogger.getRealTimeMetrics();
  }

  /**
   * ã‚¯ã‚¨ãƒªåˆ†æå±¥æ­´å–å¾—ï¼ˆå¤–éƒ¨APIç”¨ï¼‰
   */
  async getQueryAnalysisHistory(date: string, limit: number = 100): Promise<any> {
    if (!this.redisLogger) {
      return [];
    }
    
    return await this.redisLogger.getQueryAnalysisHistory(date, limit);
  }

  /**
   * æ—¥æ¬¡ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆå–å¾—ï¼ˆå¤–éƒ¨APIç”¨ï¼‰
   */
  async getDailyCostReport(date: string): Promise<any> {
    if (!this.redisLogger) {
      return { error: 'Redis Logger not available' };
    }
    
    const metrics = await this.redisLogger.getRealTimeMetrics();
    return metrics.dailyCosts;
  }

  /**
   * Redisçµ±è¨ˆæƒ…å ±å–å¾—ï¼ˆUpstashå¯¾å¿œï¼‰
   */
  async getRedisStats(): Promise<any> {
    if (!this.redisLogger) {
      return { 
        error: 'Redis Logger not available',
        service_type: 'none',
        connection_status: 'disabled'
      };
    }

    try {
      // UpstashRedisLoggerã‹ã©ã†ã‹ã‚’ç¢ºèª
      if (this.redisLogger instanceof UpstashRedisLogger) {
        return await this.redisLogger.getUpstashStats();
      } else {
        // ãƒ­ãƒ¼ã‚«ãƒ«Redisã®å ´åˆ
        return {
          connection_status: 'connected',
          service_type: 'local_redis',
          features: [
            'local_storage',
            'standard_redis_api',
            'basic_metrics'
          ]
        };
      }
    } catch (error) {
      return {
        error: 'Failed to get Redis stats',
        connection_status: 'error',
        service_type: 'unknown',
        details: error instanceof Error ? error.message : 'Unknown error'
      };
    }
  }

  // ============================================
  // IT Troubleshooting çµ±åˆãƒ¡ã‚½ãƒƒãƒ‰
  // ============================================

  /**
   * ãƒ­ã‚°è§£æãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†
   */
  async processLogAnalysis(request: LogAnalysisRequest): Promise<any> {
    console.log('[LLMOrchestrator] ğŸ” Processing log analysis request...');
    return await this.logAnalysisService.analyzeLog(request);
  }

  /**
   * é«˜åº¦ãªãƒ­ã‚°è§£æã‚’å®Ÿè¡Œ
   */
  async processAdvancedLogAnalysis(rawLogs: string, context: LogAnalysisContext): Promise<any> {
    console.log('[LLMOrchestrator] ğŸ”§ Processing advanced log analysis...');
    return await this.advancedLogAnalyzer.analyzeUserLogs(rawLogs, context);
  }

  /**
   * å¯¾è©±å‹ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹
   */
  async startTroubleshootingSession(problemDescription: string, userId?: string): Promise<any> {
    console.log('[LLMOrchestrator] ğŸ› ï¸ Starting troubleshooting session...');
    return await this.interactiveTroubleshooter.startTroubleshootingSession(problemDescription, userId);
  }

  /**
   * ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«å›ç­”
   */
  async respondToTroubleshootingSession(sessionId: string, userResponse: string): Promise<any> {
    console.log('[LLMOrchestrator] ğŸ’¬ Responding to troubleshooting session...');
    
    // ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’å–å¾—ã—ã¦é©åˆ‡ãªå‡¦ç†ã‚’åˆ¤å®š
    const session = this.interactiveTroubleshooter.getSessionStatus(sessionId);
    if (!session) {
      throw new Error(`Session ${sessionId} not found`);
    }

    // ç¾åœ¨ã¯ã‚·ãƒ³ãƒ—ãƒ«ãªè¨ºæ–­å®Ÿè¡Œã‚’è¡Œã†
    // TODO: å°†æ¥çš„ã«ã¯ã‚ˆã‚Šè©³ç´°ãªå¯¾è©±å‡¦ç†ã‚’å®Ÿè£…
    return await this.interactiveTroubleshooter.performDiagnosis(sessionId);
  }

  /**
   * ã‚³ãƒãƒ³ãƒ‰ã®å®‰å…¨æ€§ã‚’è©•ä¾¡
   */
  async assessCommandSafety(command: string, context: any): Promise<any> {
    console.log('[LLMOrchestrator] ğŸ›¡ï¸ Assessing command safety...');
    return await this.safeExecutionManager.assessCommandSafety(command);
  }

  /**
   * ITçµ±åˆã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆæƒ…å ±å–å¾—
   */
  getITSystemStats(): any {
    return {
      troubleshooting_services: {
        log_analysis: !!this.logAnalysisService,
        interactive_troubleshooter: !!this.interactiveTroubleshooter,
        advanced_log_analyzer: !!this.advancedLogAnalyzer,
        safe_execution_manager: !!this.safeExecutionManager
      },
      llm_orchestration: {
        total_models: this.clients.size,
        tiers_available: [0, 1, 2, 3],
        collaborative_coding: true,
        cost_management: !!this.costManagement
      },
      integration_status: 'fully_integrated'
    };
  }

  // ============================================
  // CLI ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ç®¡ç†ãƒ¡ã‚½ãƒƒãƒ‰ - REMOVED
  // All CLI functionality moved to ToolOrchestratorService
  // ============================================
  
  // processCLIRequest, selectOptimalCLI, startCLISession, 
  // getCLIStats, switchToGeminiCLI methods removed
  // Use ToolOrchestratorService for all CLI operations
}