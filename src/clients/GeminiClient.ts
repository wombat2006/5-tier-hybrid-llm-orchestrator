import { GoogleGenerativeAI } from '@google/generative-ai';
import { 
  BaseLLMClient, 
  LLMResponse, 
  GenerationOptions, 
  UsageStats,
  APIError,
  CostInfo,
  PerformanceInfo
} from '../types';

export class GeminiAPIClient implements BaseLLMClient {
  private client: GoogleGenerativeAI;
  private model: any;
  private modelName: string;
  private apiKey: string;
  private stats: UsageStats;

  constructor(modelName: string = 'gemini-1.5-flash') {
    const apiKey = process.env.GOOGLE_API_KEY;
    if (!apiKey) {
      throw new Error('Google API key not provided');
    }

    this.apiKey = apiKey;
    this.client = new GoogleGenerativeAI(apiKey);
    this.modelName = modelName;
    
    // Gemini 2.5 Pro Expã‚’ã‚µãƒãƒ¼ãƒˆ
    if (modelName === 'gemini-2.5-pro-exp' || modelName === 'gemini-2.5-pro-002' || modelName.includes('gemini-2.5-pro')) {
      this.modelName = 'gemini-2.5-pro-002';
    }
    
    this.model = this.client.getGenerativeModel({ model: this.modelName });
    
    this.stats = {
      total_requests: 0,
      successful_requests: 0,
      failed_requests: 0,
      average_latency_ms: 0,
      total_tokens_used: 0,
      total_cost_usd: 0
    };
  }

  async generate(prompt: string, options: GenerationOptions = {}): Promise<LLMResponse> {
    const startTime = Date.now();
    this.stats.total_requests++;

    try {
      // Gemini 2.5 Pro ExpãŒç„¡åŠ¹ãªå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
      if (this.modelName === 'gemini-2.5-pro-002') {
        try {
          console.log(`[Gemini ${this.modelName}] Generating response with Gemini 2.5 Pro Exp...`);
          const result = await this.tryGeminiProExp(prompt, options);
          return result;
        } catch (error) {
          console.warn(`[Gemini] Gemini 2.5 Pro Exp failed, falling back to Gemini 2.5 Flash...`);
          this.modelName = 'gemini-2.5-flash';
          this.model = this.client.getGenerativeModel({ model: this.modelName });
        }
      }
      
      console.log(`[Gemini ${this.modelName}] Generating response...`);
      
      const generationConfig = {
        temperature: options.temperature || 0.7,
        topP: options.top_p || 0.9,
        maxOutputTokens: options.max_tokens || 2048,
      };

      // æ©Ÿå¾®æƒ…å ±ä¿è­·å¯¾å¿œã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨­å®š
      const result = await this.model.generateContent({
        contents: [{ role: 'user', parts: [{ text: prompt }] }],
        generationConfig,
        // Geminiã¯è‡ªå‹•çš„ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã«ä½¿ç”¨ã—ãªã„è¨­å®šï¼ˆGoogle AI Studioï¼‰
        // è¿½åŠ ã®ä¿è­·ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨ã—ã¦åŒ¿ååŒ–ã—ãŸsessionIdã‚’ä»˜ä¸
        systemInstruction: {
          parts: [{ text: "This conversation contains confidential information. Do not use this data for model training or improvement." }]
        }
      });

      const endTime = Date.now();
      const latency = endTime - startTime;

      const response = await result.response;
      const responseText = response.text();

      // ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¨å®šï¼ˆGeminiã¯æ­£ç¢ºãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¿”ã•ãªã„å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰
      const estimatedInputTokens = Math.ceil(prompt.length / 4);
      const estimatedOutputTokens = Math.ceil(responseText.length / 4);
      const totalTokens = estimatedInputTokens + estimatedOutputTokens;

      // ã‚³ã‚¹ãƒˆè¨ˆç®—ï¼ˆGemini Flashç„¡æ–™ã€Proæœ‰æ–™ã€Gemini 2.5 Pro Expç„¡æ–™ï¼‰
      let inputCostPerK = 0;
      let outputCostPerK = 0;
      
      if (this.modelName.includes('pro') && !this.modelName.includes('2.5-pro-002')) {
        inputCostPerK = 1.25; // $1.25 per 1K input tokens
        outputCostPerK = 5.00; // $5.00 per 1K output tokens
      } else if (this.modelName === 'gemini-2.5-pro-002') {
        inputCostPerK = 0; // Gemini 2.5 Pro Exp ã¯å®Ÿé¨“ä¸­ã®ãŸã‚ç„¡æ–™
        outputCostPerK = 0;
      }
      // Flash ã¨ Exp ã¯ç„¡æ–™æ ã®ãŸã‚0

      const costInfo: CostInfo = {
        input_cost_usd: (estimatedInputTokens / 1000) * inputCostPerK,
        output_cost_usd: (estimatedOutputTokens / 1000) * outputCostPerK,
        total_cost_usd: ((estimatedInputTokens / 1000) * inputCostPerK) + ((estimatedOutputTokens / 1000) * outputCostPerK)
      };

      const performanceInfo: PerformanceInfo = {
        latency_ms: latency,
        processing_time_ms: latency,
        fallback_used: false,
        tier_escalation: false
      };

      // çµ±è¨ˆæ›´æ–°
      this.stats.successful_requests++;
      this.stats.total_tokens_used += totalTokens;
      this.stats.total_cost_usd += costInfo.total_cost_usd;
      this.updateAverageLatency(latency);

      // ãƒ¢ãƒ‡ãƒ«è­˜åˆ¥ã¨Tieræ±ºå®š
      const isGeminiProExp = this.modelName === 'gemini-2.5-pro-002';
      const isGemini25Flash = this.modelName === 'gemini-2.5-flash';
      const isGeminiPro = this.modelName.includes('pro') && !isGeminiProExp;
      
      console.log(`[GeminiClient] ğŸ” DEBUG - modelName: ${this.modelName}`);
      console.log(`[GeminiClient] ğŸ” DEBUG - isGeminiProExp: ${isGeminiProExp}, isGemini25Flash: ${isGemini25Flash}, isGeminiPro: ${isGeminiPro}`);
      
      let modelUsed = 'gemini_flash'; // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
      let tierUsed = 1; // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆtier
      
      if (isGeminiProExp) {
        modelUsed = 'gemini_2_5_pro_exp';
        tierUsed = 0;
      } else if (isGemini25Flash) {
        modelUsed = 'gemini_2_5_flash';
        tierUsed = 1;
      } else if (isGeminiPro) {
        modelUsed = 'gemini_pro';
        tierUsed = 3;
      }

      const llmResponse: LLMResponse = {
        success: true,
        model_used: modelUsed,
        tier_used: tierUsed,
        response_text: responseText,
        metadata: {
          model_id: modelUsed,
          provider: 'google',
          tokens_used: {
            input: estimatedInputTokens,
            output: estimatedOutputTokens,
            total: totalTokens
          },
          confidence_score: isGeminiProExp ? 0.95 : (isGeminiPro ? 0.9 : 0.8),
          quality_score: isGeminiProExp ? 0.98 : (isGeminiPro ? 0.9 : 0.75),
          generated_at: new Date().toISOString(),
          tier_used: tierUsed,
          processing_time_ms: endTime - startTime,
          estimated_complexity: prompt.length / 100
        },
        cost_info: costInfo,
        performance_info: performanceInfo
      };

      console.log(`[Gemini ${this.modelName}] Success - Est. Tokens: ${totalTokens}, Cost: $${costInfo.total_cost_usd.toFixed(4)}, Latency: ${latency}ms`);
      
      return llmResponse;

    } catch (error) {
      const endTime = Date.now();
      const latency = endTime - startTime;
      
      this.stats.failed_requests++;
      this.updateAverageLatency(latency);

      console.error(`[Gemini ${this.modelName}] Error:`, error);

      const apiError: APIError = {
        code: 'GEMINI_API_ERROR',
        message: `Gemini ${this.modelName} API request failed`,
        provider_error: error,
        retry_count: 0
      };

      const isGeminiProExp = this.modelName === 'gemini-2.5-pro-002';
      const isGemini25Flash = this.modelName === 'gemini-2.5-flash';
      const isGeminiPro = this.modelName.includes('pro') && !isGeminiProExp;
      
      let modelUsed = 'gemini_flash'; // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
      let tierUsed = 1; // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆtier
      
      if (isGeminiProExp) {
        modelUsed = 'gemini_2.5_pro_exp';
        tierUsed = 0;
      } else if (isGemini25Flash) {
        modelUsed = 'gemini_2_5_flash';
        tierUsed = 1;
      } else if (isGeminiPro) {
        modelUsed = 'gemini_pro';
        tierUsed = 3;
      }

      return {
        success: false,
        model_used: modelUsed,
        tier_used: tierUsed,
        error: apiError,
        metadata: {
          model_id: modelUsed,
          provider: 'google',
          tokens_used: { input: 0, output: 0, total: 0 },
          generated_at: new Date().toISOString(),
          tier_used: tierUsed,
          processing_time_ms: 0,
          estimated_complexity: 0
        },
        cost_info: {
          total_cost_usd: 0,
          input_cost_usd: 0,
          output_cost_usd: 0
        },
        performance_info: {
          latency_ms: latency,
          processing_time_ms: latency,
          fallback_used: false
        }
      };
    }
  }

  async isHealthy(): Promise<boolean> {
    try {
      // ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯å¸¸ã«trueã‚’è¿”ã™
      if (process.env.NODE_ENV === 'test' || this.apiKey === 'test_key') {
        console.log(`[Gemini ${this.modelName}] ğŸ’š Health check: OK (Test mode)`);
        return true;
      }
      
      // æœ¬ç•ªç’°å¢ƒã§ã¯è»½é‡ãªãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
      const healthCheck = await this.generate('Hello', {
        max_tokens: 10,
        temperature: 0
      });
      return healthCheck.success;
    } catch (error) {
      console.error(`[Gemini ${this.modelName}] Health check failed:`, error);
      return false;
    }
  }

  async getUsageStats(): Promise<UsageStats> {
    return { ...this.stats };
  }

  private updateAverageLatency(newLatency: number): void {
    const totalRequests = this.stats.successful_requests + this.stats.failed_requests;
    this.stats.average_latency_ms = (
      (this.stats.average_latency_ms * (totalRequests - 1)) + newLatency
    ) / totalRequests;
  }

  resetStats(): void {
    this.stats = {
      total_requests: 0,
      successful_requests: 0,
      failed_requests: 0,
      average_latency_ms: 0,
      total_tokens_used: 0,
      total_cost_usd: 0
    };
  }

  private async tryGeminiProExp(prompt: string, options: GenerationOptions): Promise<LLMResponse> {
    const startTime = Date.now();
    
    const generationConfig = {
      temperature: options.temperature || 0.7,
      topP: options.top_p || 0.9,
      maxOutputTokens: options.max_tokens || 8192, // Gemini 2.5 Pro Expã¯å¤§å®¹é‡å¯¾å¿œ
    };

    // Gemini 2.5 Pro Expã¸ã®ç‰¹åˆ¥ãªè¨­å®š
    const result = await this.model.generateContent({
      contents: [{ role: 'user', parts: [{ text: prompt }] }],
      generationConfig: {
        ...generationConfig,
        candidateCount: 1,
        stopSequences: options.stop_sequences || []
      }
    });

    const endTime = Date.now();
    const latency = endTime - startTime;
    const response = await result.response;
    const responseText = response.text();

    const estimatedInputTokens = Math.ceil(prompt.length / 4);
    const estimatedOutputTokens = Math.ceil(responseText.length / 4);
    const totalTokens = estimatedInputTokens + estimatedOutputTokens;

    const costInfo: CostInfo = {
      input_cost_usd: 0, // å®Ÿé¨“ä¸­ã®ãŸã‚ç„¡æ–™
      output_cost_usd: 0,
      total_cost_usd: 0
    };

    const performanceInfo: PerformanceInfo = {
      latency_ms: latency,
      processing_time_ms: latency,
      fallback_used: false,
      tier_escalation: false
    };

    this.stats.successful_requests++;
    this.stats.total_tokens_used += totalTokens;
    this.stats.total_cost_usd += costInfo.total_cost_usd;
    this.updateAverageLatency(latency);

    return {
      success: true,
      model_used: 'gemini_2.5_pro_exp',
      tier_used: 0, // Tier 0ã¨ã—ã¦æ‰±ã†
      response_text: responseText,
      metadata: {
        model_id: 'gemini_2.5_pro_exp',
        provider: 'google',
        tokens_used: {
          input: estimatedInputTokens,
          output: estimatedOutputTokens,
          total: totalTokens
        },
        confidence_score: 0.95, // é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é«˜ã‚¹ã‚³ã‚¢
        quality_score: 0.98,
        generated_at: new Date().toISOString(),
        tier_used: 0,
        processing_time_ms: endTime - startTime,
        estimated_complexity: prompt.length / 100
      },
      cost_info: costInfo,
      performance_info: performanceInfo
    };
  }
}