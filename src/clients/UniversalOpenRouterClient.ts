import OpenAI from 'openai';
import { 
  BaseLLMClient, 
  LLMResponse, 
  GenerationOptions, 
  UsageStats,
  APIError,
  CostInfo,
  PerformanceInfo,
  ModelConfig
} from '../types';

/**
 * UniversalOpenRouterClient - OpenRouterçµŒç”±ã§æ§˜ã€…ãªLLMãƒ¢ãƒ‡ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹
 * æ‹¡å¼µæ€§ã‚’è€ƒæ…®ã—ãŸè¨­è¨ˆã§ã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‹•çš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ å¯èƒ½
 */

export interface OpenRouterModelInfo {
  id: string;
  name: string;
  description?: string;
  context_length: number;
  pricing: {
    prompt: number;    // per 1M tokens
    completion: number; // per 1M tokens
  };
  capabilities?: string[];
  provider?: string;
}

export class UniversalOpenRouterClient implements BaseLLMClient {
  private client: OpenAI;
  private modelConfig: ModelConfig;
  private stats: UsageStats;
  private openRouterModelInfo?: OpenRouterModelInfo;

  constructor(modelConfig: ModelConfig, openRouterModelInfo?: OpenRouterModelInfo) {
    const apiKey = process.env.OPENROUTER_API_KEY;
    if (!apiKey) {
      throw new Error('OpenRouter API key not provided');
    }

    this.client = new OpenAI({
      apiKey: apiKey,
      baseURL: 'https://openrouter.ai/api/v1',
      defaultHeaders: {
        'HTTP-Referer': 'http://localhost:4000',
        'X-Title': '5-Tier Hybrid LLM System',
      }
    });
    
    this.modelConfig = modelConfig;
    this.openRouterModelInfo = openRouterModelInfo;
    
    this.stats = {
      total_requests: 0,
      successful_requests: 0,
      failed_requests: 0,
      total_tokens_used: 0,
      total_cost_usd: 0,
      average_latency_ms: 0
    };
  }

  async generate(prompt: string, options: GenerationOptions = {}): Promise<LLMResponse> {
    const startTime = Date.now();
    
    try {
      this.stats.total_requests++;

      // ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
      const systemPrompt = this.generateSystemPrompt();
      
      const requestOptions: OpenAI.Chat.Completions.ChatCompletionCreateParams = {
        model: this.modelConfig.name,
        messages: [
          ...(systemPrompt ? [{ role: 'system' as const, content: systemPrompt }] : []),
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: options.max_tokens || this.modelConfig.max_tokens || 4096,
        temperature: options.temperature ?? this.getDefaultTemperature(),
        top_p: options.top_p || 0.9
      };

      console.log(`[OpenRouter-${this.modelConfig.id}] ğŸ“¤ Sending request to ${this.modelConfig.name}...`);
      
      const response = await this.client.chat.completions.create(requestOptions);
      const endTime = Date.now();
      const latency = endTime - startTime;

      // ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã¨ã‚³ã‚¹ãƒˆè¨ˆç®—
      const usage = response.usage;
      const inputTokens = usage?.prompt_tokens || 0;
      const outputTokens = usage?.completion_tokens || 0;
      const totalTokens = usage?.total_tokens || inputTokens + outputTokens;

      const responseText = response.choices[0]?.message?.content || '';

      // å®Ÿéš›ã®OpenRouteræ–™é‡‘ã¾ãŸã¯configã®æ–™é‡‘ã‚’ä½¿ç”¨
      const inputCostPerM = this.openRouterModelInfo?.pricing.prompt || (this.modelConfig.cost_per_1k_tokens.input * 1000);
      const outputCostPerM = this.openRouterModelInfo?.pricing.completion || (this.modelConfig.cost_per_1k_tokens.output * 1000);
      
      const cost = (inputTokens / 1000000 * inputCostPerM) + (outputTokens / 1000000 * outputCostPerM);

      // çµ±è¨ˆæ›´æ–°
      this.stats.successful_requests++;
      this.stats.total_tokens_used += totalTokens;
      this.stats.total_cost_usd += cost;
      this.stats.average_latency_ms = 
        (this.stats.average_latency_ms * (this.stats.successful_requests - 1) + latency) / this.stats.successful_requests;

      const costInfo: CostInfo = {
        total_cost_usd: cost,
        input_cost_usd: inputTokens / 1000000 * inputCostPerM,
        output_cost_usd: outputTokens / 1000000 * outputCostPerM
      };

      const performanceInfo: PerformanceInfo = {
        latency_ms: latency,
        processing_time_ms: latency,
        queue_time_ms: 0
      };

      console.log(`[OpenRouter-${this.modelConfig.id}] âœ… Request completed - Tokens: ${totalTokens}, Cost: $${cost.toFixed(6)}, Latency: ${latency}ms`);

      return {
        success: true,
        response_text: responseText,
        model_used: this.modelConfig.name,
        tier_used: this.modelConfig.tier,
        cost_info: costInfo,
        performance_info: performanceInfo,
        metadata: {
          model_id: this.modelConfig.id,
          provider: 'openrouter',
          tokens_used: {
            input: inputTokens,
            output: outputTokens,
            total: totalTokens
          },
          generated_at: new Date().toISOString(),
          tier_used: this.modelConfig.tier,
          processing_time_ms: latency,
          estimated_complexity: this.estimateComplexity(prompt),
          operation_type: 'llm_generation' as const,
          openrouter_model: this.modelConfig.name,
          finish_reason: response.choices[0]?.finish_reason
        }
      };

    } catch (error) {
      const endTime = Date.now();
      const latency = endTime - startTime;

      this.stats.failed_requests++;

      console.error(`[OpenRouter-${this.modelConfig.id}] âŒ Request failed:`, error);

      const apiError: APIError = {
        code: this.extractErrorCode(error),
        message: error instanceof Error ? error.message : 'Unknown error occurred',
        provider_error: error,
        retry_count: 0
      };

      return {
        success: false,
        response_text: '',
        model_used: this.modelConfig.name,
        tier_used: this.modelConfig.tier,
        error: apiError,
        cost_info: {
          total_cost_usd: 0,
          input_cost_usd: 0,
          output_cost_usd: 0
        },
        performance_info: {
          latency_ms: latency,
          processing_time_ms: latency,
          queue_time_ms: 0
        },
        metadata: {
          model_id: this.modelConfig.id,
          provider: 'openrouter',
          tokens_used: {
            input: 0,
            output: 0,
            total: 0
          },
          generated_at: new Date().toISOString(),
          tier_used: this.modelConfig.tier,
          processing_time_ms: latency,
          estimated_complexity: 0,
          operation_type: 'llm_generation' as const,
          openrouter_model: this.modelConfig.name
        }
      };
    }
  }

  /**
   * ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
   */
  private generateSystemPrompt(): string {
    const capabilities = this.modelConfig.capabilities || [];
    const modelName = this.modelConfig.name.toLowerCase();

    // ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«
    if (capabilities.includes('coding') || modelName.includes('coder') || modelName.includes('code')) {
      return 'You are an expert coding assistant. Provide high-quality, efficient code solutions with clear explanations. Follow best practices and include appropriate error handling.';
    }

    // æ¨è«–ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«
    if (modelName.includes('reasoning') || modelName.includes('think') || capabilities.includes('reasoning')) {
      return 'You are an expert reasoning assistant. Break down complex problems step by step and provide thorough analysis with clear logical reasoning.';
    }

    // æ•°å­¦ãƒ»ç§‘å­¦ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«
    if (modelName.includes('math') || capabilities.includes('mathematics')) {
      return 'You are an expert mathematics assistant. Solve problems step by step with clear explanations and show your work.';
    }

    // æ±ç”¨ãƒ¢ãƒ‡ãƒ«
    return 'You are a helpful AI assistant. Provide accurate, concise, and useful responses.';
  }

  /**
   * ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¸©åº¦è¨­å®š
   */
  private getDefaultTemperature(): number {
    const capabilities = this.modelConfig.capabilities || [];
    const modelName = this.modelConfig.name.toLowerCase();

    // ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã¯ä½æ¸©åº¦
    if (capabilities.includes('coding') || modelName.includes('coder')) {
      return 0.1;
    }

    // æ¨è«–ã‚¿ã‚¹ã‚¯ã‚‚ä½æ¸©åº¦
    if (capabilities.includes('reasoning') || modelName.includes('reasoning')) {
      return 0.2;
    }

    // å‰µä½œã‚¿ã‚¹ã‚¯ã¯é«˜æ¸©åº¦
    if (capabilities.includes('creative_writing') || capabilities.includes('creative')) {
      return 0.8;
    }

    // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return 0.7;
  }

  /**
   * ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¤‡é›‘ã•ã‚’æ¨å®š
   */
  private estimateComplexity(prompt: string): number {
    let complexity = 1;
    
    // é•·ã•ãƒ™ãƒ¼ã‚¹ã®è¤‡é›‘ã•
    if (prompt.length > 1000) complexity += 1;
    if (prompt.length > 5000) complexity += 1;
    
    // ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®æ¤œå‡º
    if (prompt.includes('```') || prompt.includes('function') || prompt.includes('class')) {
      complexity += 1;
    }
    
    // è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    const complexKeywords = ['analyze', 'design', 'architecture', 'implement', 'debug', 'optimize'];
    if (complexKeywords.some(keyword => prompt.toLowerCase().includes(keyword))) {
      complexity += 1;
    }

    return Math.min(complexity, 5); // æœ€å¤§5
  }

  /**
   * ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã®æŠ½å‡º
   */
  private extractErrorCode(error: any): string {
    if (error?.response?.status) {
      return `HTTP_${error.response.status}`;
    }
    if (error?.code) {
      return error.code;
    }
    if (error?.name) {
      return error.name;
    }
    return 'OPENROUTER_ERROR';
  }

  async isHealthy(): Promise<boolean> {
    try {
      const response = await this.client.chat.completions.create({
        model: this.modelConfig.name,
        messages: [
          {
            role: 'user',
            content: 'Hello'
          }
        ],
        max_tokens: 5
      });
      return response && response.choices && response.choices.length > 0;
    } catch (error) {
      console.error(`[OpenRouter-${this.modelConfig.id}] Health check failed:`, error);
      return false;
    }
  }

  async getUsageStats(): Promise<UsageStats> {
    return Promise.resolve({ ...this.stats });
  }

  resetStats(): void {
    this.stats = {
      total_requests: 0,
      successful_requests: 0,
      failed_requests: 0,
      total_tokens_used: 0,
      total_cost_usd: 0,
      average_latency_ms: 0
    };
  }

  getModelName(): string {
    return this.modelConfig.name;
  }

  getProvider(): string {
    return 'openrouter';
  }

  getModelConfig(): ModelConfig {
    return { ...this.modelConfig };
  }

  getOpenRouterModelInfo(): OpenRouterModelInfo | undefined {
    return this.openRouterModelInfo ? { ...this.openRouterModelInfo } : undefined;
  }

  // ç‰¹æ®Šç”¨é€”ãƒ¡ã‚½ãƒƒãƒ‰
  async generateWithCustomSystem(prompt: string, systemPrompt: string, options: GenerationOptions = {}): Promise<LLMResponse> {
    const originalGenerate = this.generate;
    
    // ä¸€æ™‚çš„ã«ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰æ›´
    const tempGenerate = async (userPrompt: string, opts: GenerationOptions = {}) => {
      const startTime = Date.now();
      
      try {
        const requestOptions: OpenAI.Chat.Completions.ChatCompletionCreateParams = {
          model: this.modelConfig.name,
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: userPrompt }
          ],
          max_tokens: opts.max_tokens || this.modelConfig.max_tokens || 4096,
          temperature: opts.temperature ?? this.getDefaultTemperature(),
          top_p: opts.top_p || 0.9
        };

        const response = await this.client.chat.completions.create(requestOptions);
        // ... åŒæ§˜ã®å‡¦ç†ã‚’original generateã‹ã‚‰æµç”¨
        
        return originalGenerate.call(this, userPrompt, opts);
      } catch (error) {
        return originalGenerate.call(this, userPrompt, opts);
      }
    };

    return tempGenerate(prompt, options);
  }
}