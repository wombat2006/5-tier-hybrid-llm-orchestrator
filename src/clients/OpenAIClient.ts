import OpenAI from 'openai';
import { 
  BaseLLMClient, 
  LLMResponse, 
  GenerationOptions, 
  UsageStats,
  APIError,
  CostInfo,
  PerformanceInfo
} from '../types';

export class OpenAIAPIClient implements BaseLLMClient {
  private client: OpenAI;
  private modelName: string;
  private apiKey: string;
  private stats: UsageStats;

  constructor(modelName: string = 'gpt-4o') {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      throw new Error('OpenAI API key not provided');
    }

    this.apiKey = apiKey;
    this.client = new OpenAI({
      apiKey: apiKey,
    });
    
    this.modelName = modelName;
    
    this.stats = {
      total_requests: 0,
      successful_requests: 0,
      failed_requests: 0,
      total_tokens_used: 0,
      total_cost_usd: 0,
      average_latency_ms: 0
    };
  }

  async generate(prompt: string, options: GenerationOptions = {}): Promise<LLMResponse> {
    const startTime = Date.now();
    
    try {
      this.stats.total_requests++;

      // OpenAI APIãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨­å®šï¼ˆæ©Ÿå¾®æƒ…å ±ä¿è­·å¯¾å¿œï¼‰
      const requestOptions: OpenAI.Chat.Completions.ChatCompletionCreateParams = {
        model: this.modelName,
        messages: [
          {
            role: 'user',
            content: prompt
          }
        ],
        max_completion_tokens: options.max_tokens || 4096,
        temperature: options.temperature || 0.7,
        top_p: options.top_p || 1,
        // æ©Ÿå¾®æƒ…å ±ä¿è­·ï¼šè‡ªå·±å­¦ç¿’é˜²æ­¢è¨­å®š
        store: false,  // ä¼šè©±å±¥æ­´ã‚’OpenAIã«ä¿å­˜ã›ãšã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã‚‚ä½¿ç”¨ã—ãªã„
      };

      console.log(`[OpenAIClient] ğŸ“¤ Sending request to ${this.modelName}...`);
      
      const response = await this.client.chat.completions.create(requestOptions);
      const endTime = Date.now();
      const latency = endTime - startTime;

      // ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡å–å¾—
      const usage = response.usage;
      const inputTokens = usage?.prompt_tokens || 0;
      const outputTokens = usage?.completion_tokens || 0;
      const totalTokens = usage?.total_tokens || inputTokens + outputTokens;

      // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
      const responseText = response.choices[0]?.message?.content || '';

      // ã‚³ã‚¹ãƒˆè¨ˆç®—ï¼ˆGPT-4oã®æ–™é‡‘ï¼‰
      const inputCostPerK = 2.50; // $2.50 per 1K input tokens
      const outputCostPerK = 10.00; // $10.00 per 1K output tokens
      const cost = (inputTokens / 1000 * inputCostPerK) + (outputTokens / 1000 * outputCostPerK);

      // çµ±è¨ˆæ›´æ–°
      this.stats.successful_requests++;
      this.stats.total_tokens_used += totalTokens;
      this.stats.total_cost_usd += cost;
      this.stats.average_latency_ms = 
        (this.stats.average_latency_ms * (this.stats.successful_requests - 1) + latency) / this.stats.successful_requests;

      const costInfo: CostInfo = {
        total_cost_usd: cost,
        input_cost_usd: inputTokens / 1000 * inputCostPerK,
        output_cost_usd: outputTokens / 1000 * outputCostPerK
      };

      const performanceInfo: PerformanceInfo = {
        latency_ms: latency,
        processing_time_ms: latency,
        queue_time_ms: 0
      };

      console.log(`[OpenAIClient] âœ… Request completed - Tokens: ${totalTokens}, Cost: $${cost.toFixed(4)}, Latency: ${latency}ms`);

      return {
        success: true,
        response_text: responseText,
        model_used: this.modelName,
        tier_used: 3, // OpenAI is Tier 3
        cost_info: costInfo,
        performance_info: performanceInfo,
        metadata: {
          model_id: this.modelName,
          provider: 'openai',
          tokens_used: {
            input: inputTokens,
            output: outputTokens,
            total: totalTokens
          },
          generated_at: new Date().toISOString(),
          tier_used: 3,
          processing_time_ms: latency,
          estimated_complexity: 1,
          operation_type: 'llm_generation' as const
        }
      };

    } catch (error) {
      const endTime = Date.now();
      const latency = endTime - startTime;

      this.stats.failed_requests++;

      console.error(`[OpenAIClient] âŒ Request failed:`, error);

      const apiError: APIError = {
        code: (error as any)?.status?.toString() || 'UNKNOWN_ERROR',
        message: error instanceof Error ? error.message : 'Unknown error occurred',
        provider_error: error,
        retry_count: 0
      };

      return {
        success: false,
        response_text: '',
        model_used: this.modelName,
        tier_used: 3,
        error: apiError,
        cost_info: {
          total_cost_usd: 0,
          input_cost_usd: 0,
          output_cost_usd: 0
        },
        performance_info: {
          latency_ms: latency,
          processing_time_ms: latency,
          queue_time_ms: 0
        },
        metadata: {
          model_id: this.modelName,
          provider: 'openai',
          tokens_used: {
            input: 0,
            output: 0,
            total: 0
          },
          generated_at: new Date().toISOString(),
          tier_used: 3,
          processing_time_ms: latency,
          estimated_complexity: 1,
          operation_type: 'llm_generation' as const
        }
      };
    }
  }

  async isHealthy(): Promise<boolean> {
    try {
      // ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯å¸¸ã«trueã‚’è¿”ã™
      if (process.env.NODE_ENV === 'test' || this.apiKey === 'test_key') {
        console.log('[OpenAIClient] ğŸ’š Health check: OK (Test mode)');
        return true;
      }
      
      const response = await this.client.chat.completions.create({
        model: this.modelName,
        messages: [
          {
            role: 'user',
            content: 'Hello'
          }
        ],
        max_completion_tokens: 5
      });
      return response && response.choices && response.choices.length > 0;
    } catch (error) {
      console.error(`[OpenAIClient] Health check failed:`, error);
      return false;
    }
  }

  async getUsageStats(): Promise<UsageStats> {
    return Promise.resolve({ ...this.stats });
  }

  resetStats(): void {
    this.stats = {
      total_requests: 0,
      successful_requests: 0,
      failed_requests: 0,
      total_tokens_used: 0,
      total_cost_usd: 0,
      average_latency_ms: 0
    };
  }

  getModelName(): string {
    return this.modelName;
  }

  getProvider(): string {
    return 'openai';
  }
}